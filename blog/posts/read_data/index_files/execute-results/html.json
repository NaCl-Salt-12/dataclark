{
  "hash": "8fafe427dace9dd03a3cfd166899ba3a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Introduction to Polars Pt. 2\"\nsubtitle: \"Reading in Data\"\nauthor: \"Nathaniel Clark\"\ndate: \"2025-04-15\"\ncategories: [polars,python,Introduction to Polars]\nimage: \"read.jpg\"  # You can add an image file here\n---\n\nIn this segement, we'll explore how to read data with Polars using various file formats including CSVs, JSONs, Excel spreadsheets, and Parquet files, whether stored locally or accessed via the web. Note that we won't be covering cloud storage or database connections in this tutorial.\n\n\n## Importing Data with Polars\n\n:::: {.columns}\n\n::: {.column width=\"55%\" style=\"padding-right: 30px;\"}\nPolars provides robust capabilities for importing data from various sources including CSVs, JSONs, Excel spreadsheets, Parquet files, cloud storage solutions (AWS, Azure, and Google Cloud), and databases.\n\nThe importing methods follow a consistent pattern across file types, making it easy to work with different data formats.\n:::\n\n::: {.column width=\"35%\"}\n![](read.jpg)\n:::\n\n::::\n\n\n### CSV Files\n\nThe basic syntax for reading a CSV file is:\n```python\npl.read_csv(\"path/to/data.csv\")\n```\n\nAlternatively, you can also read CSV files directly from the internet:\n```python\npl.read_csv(\"https://example.com/path/to/your/file.csv\")\n```\nThis capability to read files directly from URLs also works with all the file import methods we'll cover below.\n\n\nThis function offers numerous parameters to handle different CSV formats and configurations. For more information read the [documentation](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_csv.html).\n\n::: {#76a5ad6f .cell execution_count=1}\n``` {.python .cell-code}\nimport polars as pl\n\ndf_csv = pl.read_csv(\"example.csv\", try_parse_dates=True)\ndf_csv.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>first_name</th><th>last_name</th><th>email</th><th>purchase_date</th><th>product</th><th>quantity</th><th>price</th><th>total</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>date</td><td>str</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1</td><td>&quot;John&quot;</td><td>&quot;Doe&quot;</td><td>&quot;john.doe@example.com&quot;</td><td>2023-01-15</td><td>&quot;Laptop&quot;</td><td>1</td><td>1299.99</td><td>1299.99</td></tr><tr><td>2</td><td>&quot;Jane&quot;</td><td>&quot;Smith&quot;</td><td>&quot;jane.smith@example.com&quot;</td><td>2023-01-16</td><td>&quot;Smartphone&quot;</td><td>2</td><td>699.95</td><td>1399.9</td></tr><tr><td>3</td><td>&quot;Robert&quot;</td><td>&quot;Johnson&quot;</td><td>&quot;rob.j@example.com&quot;</td><td>2023-01-18</td><td>&quot;Headphones, Wireless&quot;</td><td>3</td><td>89.99</td><td>269.97</td></tr><tr><td>4</td><td>&quot;Sarah&quot;</td><td>&quot;Williams&quot;</td><td>&quot;sarah.w@example.com&quot;</td><td>2023-01-20</td><td>&quot;Monitor&quot;</td><td>1</td><td>249.5</td><td>249.5</td></tr><tr><td>5</td><td>&quot;Michael&quot;</td><td>&quot;Brown&quot;</td><td>&quot;michael.b@example.com&quot;</td><td>2023-01-22</td><td>&quot;Keyboard&quot;</td><td>2</td><td>59.99</td><td>119.98</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n### JSON Files\n\nReading JSON files follows a similar pattern. The basic syntax is:\n```python\npl.read_json(\"docs/data/path.json\")\n```\n\nJSON files have a more standardized structure than CSVs, so the reading process requires fewer configuration parameters. Polars handles JSON parsing efficiently with minimal setup. For advanced options and configurations, consult the official [documentation](https://docs.pola.rs/api/python/stable/reference/api/polars.read_json.html).\n\n```python\ndf_json = pl.read_json(\"basketball.json\")\n\ndf_json\n```\n\n### Excel \n\nPolars doesn't include a native Excel reader. Instead, it leverages external libraries like fastexcel, xlsx2csv, or openpyxl to parse Excel files into Polars-compatible formats. Among these options, Polars recommends fastexcel for optimal performance. \n\nWhile it's generally better to avoid using Excel files where possible (you can usually export as CSV directly from Excel), reading Excel files is straightforward with the right dependencies installed.\n\nBefore attempting to read Excel files, make sure you have at least one of these libraries installed:\n```bash\n $ pip install fastexcel xlsx2csv openpyxl\n ```\n\n\nThe basic syntax for reading an Excel file with Polars is:\n```python\npl.read_excel(\"path/to/data.xlsx\")\n```\n\nIf your Excel file contains multiple sheets, you can specify which one to read using the `sheet_name` parameter:\n```python\ndf = pl.read_excel(\"path/to/data.xlsx\", sheet_name=\"example\")\n```\n\nFor additional Excel reading options and parameters, refer to the [Polars Excel documentation](https://docs.pola.rs/api/python/stable/reference/api/polars.read_excel.html), which covers sheet selection, range specification, and handling of complex Excel files.\n\n::: {#e8b3c8f6 .cell execution_count=2}\n``` {.python .cell-code}\ndf_xlsx = pl.read_excel(\"penguins.xlsx\", sheet_name=\"Dream Island\")\n\ndf_xlsx.tail(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>species</th><th>island</th><th>bill_length_mm</th><th>bill_depth_mm</th><th>flipper_length_mm</th><th>body_mass_g</th><th>sex</th><th>year</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Chinstrap&quot;</td><td>&quot;Dream&quot;</td><td>55.8</td><td>19.8</td><td>207</td><td>4000</td><td>&quot;male&quot;</td><td>2009</td></tr><tr><td>&quot;Chinstrap&quot;</td><td>&quot;Dream&quot;</td><td>43.5</td><td>18.1</td><td>202</td><td>3400</td><td>&quot;female&quot;</td><td>2009</td></tr><tr><td>&quot;Chinstrap&quot;</td><td>&quot;Dream&quot;</td><td>49.6</td><td>18.2</td><td>193</td><td>3775</td><td>&quot;male&quot;</td><td>2009</td></tr><tr><td>&quot;Chinstrap&quot;</td><td>&quot;Dream&quot;</td><td>50.8</td><td>19.0</td><td>210</td><td>4100</td><td>&quot;male&quot;</td><td>2009</td></tr><tr><td>&quot;Chinstrap&quot;</td><td>&quot;Dream&quot;</td><td>50.2</td><td>18.7</td><td>198</td><td>3775</td><td>&quot;female&quot;</td><td>2009</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThis example spreadsheet can be accessed via [this Google Sheets link](https://docs.google.com/spreadsheets/d/1aFu8lnD_g0yjF5O-K6SFgSEWiHPpgvFCF0NY9D6LXnY/edit?gid=0#gid=0).\n\n### Parquet Files\n\nParquet is a columnar storage format designed for efficient data analytics. It provides excellent compression and fast query performance, making it a popular choice for data science workflows. Polars includes native, high-performance support for reading Parquet files.\n\nThe basic syntax for reading a Parquet file is:\n\n```python\npl.read_parquet(\"path/to/data.parquet\")\n```\n\n::: {#df48230b .cell execution_count=3}\n``` {.python .cell-code}\ndf_par = pl.read_parquet(\"finance.parquet\")\ndf_par.sample(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (4, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>ticker</th><th>open</th><th>high</th><th>low</th><th>close</th><th>volume</th><th>pe_ratio</th><th>dividend_yield</th><th>market_cap</th></tr><tr><td>datetime[ns]</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2024-10-03 00:00:00</td><td>&quot;GOOG&quot;</td><td>260.26</td><td>260.75</td><td>258.91</td><td>260.7</td><td>4574773</td><td>38.36</td><td>0.019</td><td>1.1248e10</td></tr><tr><td>2024-12-30 00:00:00</td><td>&quot;META&quot;</td><td>719.67</td><td>724.31</td><td>717.31</td><td>722.55</td><td>6903040</td><td>12.05</td><td>0.0287</td><td>3.1648e10</td></tr><tr><td>2024-08-05 00:00:00</td><td>&quot;MSFT&quot;</td><td>197.56</td><td>198.92</td><td>197.33</td><td>198.43</td><td>1807180</td><td>11.17</td><td>0.02</td><td>2.4715e9</td></tr><tr><td>2025-02-06 00:00:00</td><td>&quot;AAPL&quot;</td><td>313.37</td><td>317.63</td><td>313.88</td><td>314.54</td><td>3111621</td><td>34.75</td><td>0.0156</td><td>5.7907e9</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n### Importing Mutiple files\n\nFor situations where you need to combine data from multiple files into a single DataFrame, Polars offers straightforward approaches. While the syntax is relatively simple, the implementation may vary depending on your specific file organization.\n\nWhen working with multiple files of the same type and similar naming patterns in a single directory, Polars supports glob pattern matching:\n\n```python\npl.read_filetype(\"path/to/data/my_many_files_*.filetype\")\n```\nFor files with different names but the same format, placing them in a single directory allows you to use wildcard patterns to import them all at once:\n\n```python\npl.read_filetype(\"path/to/data/import/*.filetype\")\n```\nAlternatively, for files located in different directories or even on different servers, you can provide a list of filepaths or URLs:\n\n```python\npl.read_filetype([\n    \"path/to/first/file.filetype\",\n    \"path/to/second/file.filetype\",\n    \"another/location/file.filetype\"\n])\n```\nIf you're working with different file types that share the same schema (identical columns and datatypes) and want to combine them into a single DataFrame, you'll need to read each file individually and then concatenate them. Polars makes this process straightforward with its `concat` function, which can merge DataFrames regardless of their original file formats.\n\n```python\n# Read files of different formats\ndf1 = pl.read_csv(\"path/to/file.csv\")\ndf2 = pl.read_parquet(\"path/to/file.parquet\")\ndf3 = pl.read_json(\"path/to/file.json\")\n\n# Concatenate into a single DataFrame\ncombined_df = pl.concat([df1, df2, df3], how=\"vertical\")\n```\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}