{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Introduction to Polars Pt. 1\"\n",
        "subtitle: \"Dataframes and Series\"\n",
        "author: \"Nathaniel Clark\"\n",
        "date: \"2025-04-04\"\n",
        "categories: [polars,python,Introduction to Polars]\n",
        "image: \"polars.jpg\"  # You can add an image file here\n",
        "---\n",
        "\n",
        "\n",
        "Recently I took a data science course that heavily utilized pandas for data manipulation in Python. Being curious about emerging technologies, I asked my instructor if I could use Polars instead. He agreed but cautioned that he wouldn't have supporting materials for my alternative choice.\n",
        "\n",
        "Throughout the course, I discovered a lack of beginner-friendly Polars resources for those new to data science. I primarily relied on official documentation, which presented some challenges along the way. Despite these obstacles, I persevered, learned a great deal, and successfully completed the class.\n",
        "\n",
        "I'm writing this post to share what I've learned about Polars and provide an accessible introduction for newcomers to data manipulation libraries, especially those with limited prior experience in the field.\n",
        "\n",
        "## What is Polars?\n",
        "\n",
        "![](polars.jpg)\n",
        "\n",
        "Polars is a modern data manipulation library for Python and Rust designed as a high-performance alternative to pandas, especially for large datasets. It features syntax that's both human-readable and similar to R's data manipulation paradigms. Polars stands out for three main reasons:\n",
        "\n",
        "- **Performance**: Built in Rust, Polars delivers exceptional speed through parallel processing by default and a sophisticated query optimizer that analyzes and improves execution plans.\n",
        "- **Memory efficiency**: Using a columnar memory format rather than row-based storage, Polars efficiently handles larger-than-memory datasets and performs operations with minimal memory overhead.\n",
        "- **Lazy evaluation**: Polars supports both eager and lazy execution modes. The lazy API builds optimized query plans before execution, similar to database query planners, resulting in more efficient data processing pipelines.\n",
        "\n",
        "\n",
        "## DataFrames and Series\n",
        "\n",
        "### Series\n",
        "\n",
        "The two most common data structures in Polars are DataFrames and Series. Series are one-dimensional data structures where all elements share the same datatype. Think of a Series as a single column in a table - it's essentially a named array of data.\n",
        "\n",
        "Polars supports a variety of data types that fall into these major categories:\n",
        "\n",
        "- **Numeric**: Signed integers, unsigned integers, floating point numbers, and decimals\n",
        "- **Nested**: Lists, structs, and arrays for handling complex data\n",
        "- **Temporal**: Dates, datetimes,and times for working with time-based data\n",
        "- **Miscellaneous**: Strings, binary data, Booleans, categoricals, enums, and objects\n",
        "\n",
        "Creating a Series is straightforward with the following syntax:\n",
        "\n",
        "`pl.Series(name, values_list)`\n",
        "\n",
        "Where \"name\" is the label for your Series and \"values_list\" contains the data. Here's a simple example:"
      ],
      "id": "279619b8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "s = pl.Series(\"example\", [1, 2, 3, 4, 5])\n",
        "s"
      ],
      "id": "ba00a5bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataframes \n",
        "\n",
        "DataFrames are tabular data structures (rows and columns) composed of multiple Series, with each column representing a single Series. These are the workhorses of data analysis and what you'll use most frequently.\n",
        "\n",
        "With DataFrames, you can write powerful queries to filter, transform, aggregate, and reshape your data efficiently.\n",
        "\n",
        "DataFrames can be created in several ways:\n",
        "\n",
        "1. From a dictionary of sequences (lists, arrays)\n",
        "2. With explicit schema specification \n",
        "3. From a sequence of (name, dtype) pairs\n",
        "4. From NumPy arrays\n",
        "5. From a list of lists (row-oriented data)\n",
        "6. By converting pandas DataFrames\n",
        "7. By importing existing tabular data from CSVs, JSON, SQL, Parquet files, etc.\n",
        "\n",
        "In real-world environments, you'll typically work with preexisting data, though understanding various creation methods is valuable. We'll cover data import techniques later, but for now, here's an example of a DataFrame created from a dictionary of lists:"
      ],
      "id": "5cd28a08"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a DataFrame from a dictionary of lists\n",
        "df = pl.DataFrame({\n",
        "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
        "    \"age\": [25, 30, 35, 40],\n",
        "    \"city\": [\"New York\", \"Boston\", \"Chicago\", \"Seattle\"],\n",
        "    \"salary\": [75000, 85000, 90000, 95000]\n",
        "})\n",
        "\n",
        "df"
      ],
      "id": "9525d00b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/nathaniel/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}