[
  {
    "objectID": "website_env/lib/python3.13/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "href": "website_env/lib/python3.13/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "title": "DataClark",
    "section": "",
    "text": "Copyright (c) 2012-2023, Michael L. Waskom All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "website_env/lib/python3.13/site-packages/pandas/tests/indexes/period/test_constructors.html",
    "href": "website_env/lib/python3.13/site-packages/pandas/tests/indexes/period/test_constructors.html",
    "title": "DataClark",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "website_env/lib/python3.13/site-packages/narwhals-1.33.0.dist-info/licenses/LICENSE.html",
    "href": "website_env/lib/python3.13/site-packages/narwhals-1.33.0.dist-info/licenses/LICENSE.html",
    "title": "DataClark",
    "section": "",
    "text": "MIT License\nCopyright (c) 2024, Marco Gorelli\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n Back to top"
  },
  {
    "objectID": "website_env/lib/python3.13/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "href": "website_env/lib/python3.13/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "title": "DataClark",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "website_env/lib/python3.13/site-packages/httpcore-1.0.7.dist-info/licenses/LICENSE.html",
    "href": "website_env/lib/python3.13/site-packages/httpcore-1.0.7.dist-info/licenses/LICENSE.html",
    "title": "DataClark",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "website_env/lib/python3.13/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "href": "website_env/lib/python3.13/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "title": "DataClark",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Data Science Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Polars Pt. 2\nReading in Data\n\n \n\n\npolars\n\npython\n\nIntroduction to Polars\n\n\n \n\n\n\n\n\nApr 15, 2025\nNathaniel Clark\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Polars Pt. 1\nDataframes and Series\n\n \n\n\npolars\n\npython\n\nIntroduction to Polars\n\n\n \n\n\n\n\n\nApr 4, 2025\nNathaniel Clark\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to My Data Science Blog\n\n\n \n\n\nnews\n\nintroduction\n\n\n \n\n\n\n\n\nApr 3, 2025\nNathaniel Clark\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "blog/posts/welcome/index.html",
    "href": "blog/posts/welcome/index.html",
    "title": "Welcome to My Data Science Blog",
    "section": "",
    "text": "This is my first blog post on my data science journey. I’ll be sharing insights, projects, and learnings here."
  },
  {
    "objectID": "blog/posts/welcome/index.html#welcome",
    "href": "blog/posts/welcome/index.html#welcome",
    "title": "Welcome to My Data Science Blog",
    "section": "",
    "text": "This is my first blog post on my data science journey. I’ll be sharing insights, projects, and learnings here."
  },
  {
    "objectID": "blog/posts/welcome/index.html#what-to-expect",
    "href": "blog/posts/welcome/index.html#what-to-expect",
    "title": "Welcome to My Data Science Blog",
    "section": "What to Expect",
    "text": "What to Expect\nIn this blog, I’ll cover:\n\nData analysis techniques\nInteresting projects I’m working on\nInsights from the data science field\nTutorials and how-to guides\n\nStay tuned for more content! ```"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Nathaniel Clark",
    "section": "",
    "text": "nathanieleclark@gmail.com | +14482007651"
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Nathaniel Clark",
    "section": "Currently",
    "text": "Currently\nStudying Data Science at Brigham Young University-Idaho\n\nSpecialized in\nPython programming, data analysis, database management, cybersecurity fundamentals\n\n\nResearch interests\nMachine learning, data visualization, database design, cybersecurity"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Nathaniel Clark",
    "section": "Education",
    "text": "Education\n\nBrigham Young University – Idaho\nBachelor of Science, Data Science\nSeptember 2024 – Present\nGPA: 3.46"
  },
  {
    "objectID": "resume.html#awards",
    "href": "resume.html#awards",
    "title": "Nathaniel Clark",
    "section": "Awards",
    "text": "Awards\n2025 3rd Place, Cybersecurity Category, USU Hackathon, Utah State University"
  },
  {
    "objectID": "resume.html#technical-skills",
    "href": "resume.html#technical-skills",
    "title": "Nathaniel Clark",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nLanguages: Python, R, C#, SQL\n\nData Science: Data cleaning, data visualization (ggplot2, Lets-Plot), data wrangling (Polars, Tidyverse), machine learning (TensorFlow, scikit-learn, XGBoost)"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Nathaniel Clark",
    "section": "Experience",
    "text": "Experience\n\nData Science Tutor\nBrigham Young University – Idaho\nApril 2025 – Present\n- Provided one-on-one support for data science concepts: analysis, programming, visualizations, and statistics\n- Tutored students in Python, R, data analysis, and machine learning\n- Assisted students in a walk-in lab covering a variety of statistics and data science courses\n\n\nProject Manager – Agentic Grader\nBrigham Young University – Idaho\nApril 2025 – July 2025\n- Led a team to develop an automated grading system for a semester-long project\n- Utilized tools like n8n and LLM APIs\n\n\nCourse Designer – Data Engineering\nBrigham Young University – Idaho\nApril 2025 – July 2025\n- Designed curriculum and assignments for a data engineering course\n- Used Snowflake, Airflow, and Python for teaching automation and data pipeline concepts\n\n\nTeaching Assistant\nBrigham Young University – Idaho\nDecember 2024 – July 2025\n- Graded and created assignments for courses including Fundamentals of Cybersecurity, Introduction to Databases, and SQL\n- Mentored students on technical concepts and problem-solving approaches\n\n\nFull-Time Missionary\nThe Church of Jesus Christ of Latter-day Saints – Salt Lake City, UT\nJanuary 2022 – December 2024\n- Developed interpersonal and communication skills through community outreach\n- Worked 60–80 hours per week in challenging conditions, maintaining a positive and goal-focused mindset"
  },
  {
    "objectID": "resume.html#projects",
    "href": "resume.html#projects",
    "title": "Nathaniel Clark",
    "section": "Projects",
    "text": "Projects\n\nMetals in Mistborn: Text wrangling a visualization of Mistborn Final Empire, for more you can read this article or see the code here.\nDisc Golf Speed Prediction: Predicting the speed metric from a discs physical metrics with xgboost. See the article."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nathaniel Clark",
    "section": "",
    "text": "I’m a Data Science student at Brigham Young University-Idaho with expertise in Python programming, data analysis, database management, and cybersecurity fundamentals. My journey in data science combines technical skills with practical problem-solving abilities.\n\n GitHub  LinkedIn"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Nathaniel Clark",
    "section": "",
    "text": "I’m a Data Science student at Brigham Young University-Idaho with expertise in Python programming, data analysis, database management, and cybersecurity fundamentals. My journey in data science combines technical skills with practical problem-solving abilities.\n\n GitHub  LinkedIn"
  },
  {
    "objectID": "index.html#my-focus-areas",
    "href": "index.html#my-focus-areas",
    "title": "Nathaniel Clark",
    "section": "My Focus Areas",
    "text": "My Focus Areas\n\n\n Technical Skills\n\nProgramming Languages: Python, R, C#, SQL\nData Analysis: Cleaning, transformation, visualization\nDatabase Systems: Design, management, security protocols\nMachine Learning: Classification, regression, clustering\n\n\n\n Professional Interests\n\nMachine Learning & AI — Deep learning, LLMs, predictive modeling\nData Engineering — ETL pipelines, data systems, cloud infrastructure\nData Visualization — Interactive dashboards, storytelling with data\nStatistical Analysis — Hypothesis testing, regression modeling, Bayesian methods\nSoftware Development — Creating data-driven applications and tools"
  },
  {
    "objectID": "index.html#recent-blog-posts",
    "href": "index.html#recent-blog-posts",
    "title": "Nathaniel Clark",
    "section": " Recent Blog Posts",
    "text": "Recent Blog Posts\n\n\n\nIntroduction to Polars Pt. 2\n2025-04-15\n\n\n\nIntroduction to Polars Pt. 1\n2025-04-04\n\n\n\nWelcome to My Data Science Blog\n2025-04-03"
  },
  {
    "objectID": "index.html#portfolio-overview",
    "href": "index.html#portfolio-overview",
    "title": "Nathaniel Clark",
    "section": "Portfolio Overview",
    "text": "Portfolio Overview\nThis website showcases my projects and technical abilities in various aspects of data science. Browse through the sections to explore my work in data analysis, visualization, machine learning, and more.\nView My Resume Contact Me :::"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Story_Telling/project4.html",
    "href": "Story_Telling/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Story_Telling/project2.html",
    "href": "Story_Telling/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning/project5.html",
    "href": "Machine_Learning/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning/project3.html",
    "href": "Machine_Learning/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning/project1.html",
    "href": "Machine_Learning/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project4.html",
    "href": "Full_Stack/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project2.html",
    "href": "Full_Stack/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Competition/project5.html",
    "href": "Competition/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Competition/project3.html",
    "href": "Competition/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Competition/project1.html",
    "href": "Competition/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Projects/project4.html",
    "href": "Cleansing_Projects/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Projects/project2.html",
    "href": "Cleansing_Projects/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html",
    "href": "Cleansing_Exploration/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html",
    "href": "Cleansing_Exploration/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project1.html",
    "href": "Cleansing_Exploration/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html",
    "href": "Cleansing_Exploration/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html",
    "href": "Cleansing_Exploration/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Projects/project1.html",
    "href": "Cleansing_Projects/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Projects/project3.html",
    "href": "Cleansing_Projects/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Projects/project5.html",
    "href": "Cleansing_Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Competition/project2.html",
    "href": "Competition/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Competition/project4.html",
    "href": "Competition/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project1.html",
    "href": "Full_Stack/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project3.html",
    "href": "Full_Stack/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project5.html",
    "href": "Full_Stack/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning/project2.html",
    "href": "Machine_Learning/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Machine_Learning/project4.html",
    "href": "Machine_Learning/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Story_Telling/project1.html",
    "href": "Story_Telling/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Story_Telling/project3.html",
    "href": "Story_Telling/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Story_Telling/project5.html",
    "href": "Story_Telling/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "blog/posts/introduction_to_polars_pt1/index.html",
    "href": "blog/posts/introduction_to_polars_pt1/index.html",
    "title": "Introduction to Polars Pt. 1",
    "section": "",
    "text": "Recently I took a data science course that heavily utilized pandas for data manipulation in Python. Being curious about emerging technologies, I asked my instructor if I could use Polars instead. He agreed but cautioned that he wouldn’t have supporting materials for my alternative choice.\nThroughout the course, I discovered a lack of beginner-friendly Polars resources for those new to data science. I primarily relied on official documentation, which presented some challenges along the way. Despite these obstacles, I persevered, learned a great deal, and successfully completed the class.\nI’m writing this post to share what I’ve learned about Polars and provide an accessible introduction for newcomers to data manipulation libraries, especially those with limited prior experience in the field."
  },
  {
    "objectID": "blog/posts/introduction_to_polars_pt1/index.html#what-is-polars",
    "href": "blog/posts/introduction_to_polars_pt1/index.html#what-is-polars",
    "title": "Introduction to Polars Pt. 1",
    "section": "What is Polars?",
    "text": "What is Polars?\n\nPolars is a modern data manipulation library for Python and Rust designed as a high-performance alternative to pandas, especially for large datasets. It features syntax that’s both human-readable and similar to R’s data manipulation paradigms. Polars stands out for three main reasons:\n\nPerformance: Built in Rust, Polars delivers exceptional speed through parallel processing by default and a sophisticated query optimizer that analyzes and improves execution plans.\nMemory efficiency: Using a columnar memory format rather than row-based storage, Polars efficiently handles larger-than-memory datasets and performs operations with minimal memory overhead.\nLazy evaluation: Polars supports both eager and lazy execution modes. The lazy API builds optimized query plans before execution, similar to database query planners, resulting in more efficient data processing pipelines."
  },
  {
    "objectID": "blog/posts/introduction_to_polars_pt1/index.html#dataframes-and-series",
    "href": "blog/posts/introduction_to_polars_pt1/index.html#dataframes-and-series",
    "title": "Introduction to Polars Pt. 1",
    "section": "DataFrames and Series",
    "text": "DataFrames and Series\n\nSeries\nThe two most common data structures in Polars are DataFrames and Series. Series are one-dimensional data structures where all elements share the same datatype. Think of a Series as a single column in a table - it’s essentially a named array of data.\nPolars supports a variety of data types that fall into these major categories:\n\nNumeric: Signed integers, unsigned integers, floating point numbers, and decimals\nNested: Lists, structs, and arrays for handling complex data\nTemporal: Dates, datetimes,and times for working with time-based data\nMiscellaneous: Strings, binary data, Booleans, categoricals, enums, and objects\n\nCreating a Series is straightforward with the following syntax:\npl.Series(name, values_list)\nWhere “name” is the label for your Series and “values_list” contains the data. Here’s a simple example:\n\nimport polars as pl\ns = pl.Series(\"example\", [1, 2, 3, 4, 5])\ns\n\n\nshape: (5,)\n\n\n\nexample\n\n\ni64\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\n\n\nDataframes\nDataFrames are tabular data structures (rows and columns) composed of multiple Series, with each column representing a single Series. These are the workhorses of data analysis and what you’ll use most frequently.\nWith DataFrames, you can write powerful queries to filter, transform, aggregate, and reshape your data efficiently.\nDataFrames can be created in several ways:\n\nFrom a dictionary of sequences (lists, arrays)\nWith explicit schema specification\nFrom a sequence of (name, dtype) pairs\nFrom NumPy arrays\nFrom a list of lists (row-oriented data)\nBy converting pandas DataFrames\nBy importing existing tabular data from CSVs, JSON, SQL, Parquet files, etc.\n\nIn real-world environments, you’ll typically work with preexisting data, though understanding various creation methods is valuable. We’ll cover data import techniques later, but for now, here’s an example of a DataFrame created from a dictionary of lists:\n\n# Create a DataFrame from a dictionary of lists\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    \"age\": [25, 30, 35, 40],\n    \"city\": [\"New York\", \"Boston\", \"Chicago\", \"Seattle\"],\n    \"salary\": [75000, 85000, 90000, 95000]\n})\n\ndf\n\n\nshape: (4, 4)\n\n\n\nname\nage\ncity\nsalary\n\n\nstr\ni64\nstr\ni64\n\n\n\n\n\"Alice\"\n25\n\"New York\"\n75000\n\n\n\"Bob\"\n30\n\"Boston\"\n85000\n\n\n\"Charlie\"\n35\n\"Chicago\"\n90000\n\n\n\"David\"\n40\n\"Seattle\"\n95000"
  },
  {
    "objectID": "website_env/lib/python3.13/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "href": "website_env/lib/python3.13/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "title": "DataClark",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2024 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n Back to top"
  },
  {
    "objectID": "website_env/lib/python3.13/site-packages/idna-3.10.dist-info/LICENSE.html",
    "href": "website_env/lib/python3.13/site-packages/idna-3.10.dist-info/LICENSE.html",
    "title": "DataClark",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "website_env/lib/python3.13/site-packages/cffi/recompiler.html",
    "href": "website_env/lib/python3.13/site-packages/cffi/recompiler.html",
    "title": "DataClark",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "website_env/lib/python3.13/site-packages/numpy/random/LICENSE.html",
    "href": "website_env/lib/python3.13/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "website_env/lib/python3.13/site-packages/pandas/tests/indexes/period/test_indexing.html",
    "href": "website_env/lib/python3.13/site-packages/pandas/tests/indexes/period/test_indexing.html",
    "title": "DataClark",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "website_env/lib/python3.13/site-packages/pandas/tests/indexes/datetimes/test_indexing.html",
    "href": "website_env/lib/python3.13/site-packages/pandas/tests/indexes/datetimes/test_indexing.html",
    "title": "DataClark",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "blog/posts/read_write_view/index.html",
    "href": "blog/posts/read_write_view/index.html",
    "title": "Introduction to Polars Pt. 2",
    "section": "",
    "text": "In this segement, we’ll explore how to read and write data with Polars using various file formats including CSVs, JSONs, Excel spreadsheets, and Parquet files, whether stored locally or accessed via the web. We’ll also cover useful techniques for viewing and inspecting your imported data. Note that we won’t be covering cloud storage or database connections in this tutorial."
  },
  {
    "objectID": "blog/posts/read_write_view/index.html#local-files",
    "href": "blog/posts/read_write_view/index.html#local-files",
    "title": "Introduction to Polars Pt. 2",
    "section": "Local Files",
    "text": "Local Files\nImporting from files on your local machine is straightforward - you simply provide the file path to the appropriate reader function.\n\nCSV Files\nThe basic syntax for reading a CSV file is: pl.read_csv(\"path/to/data.csv\")\nThis function offers numerous parameters to handle different CSV formats and configurations. For more information read the documentation.\n\nimport polars as pl\n\ndf_csv = pl.read_csv(\"example.csv\", try_parse_dates=True)\ndf_csv.head(5)\n\n\nshape: (5, 9)\n\n\n\nid\nfirst_name\nlast_name\nemail\npurchase_date\nproduct\nquantity\nprice\ntotal\n\n\ni64\nstr\nstr\nstr\ndate\nstr\ni64\nf64\nf64\n\n\n\n\n1\n\"John\"\n\"Doe\"\n\"john.doe@example.com\"\n2023-01-15\n\"Laptop\"\n1\n1299.99\n1299.99\n\n\n2\n\"Jane\"\n\"Smith\"\n\"jane.smith@example.com\"\n2023-01-16\n\"Smartphone\"\n2\n699.95\n1399.9\n\n\n3\n\"Robert\"\n\"Johnson\"\n\"rob.j@example.com\"\n2023-01-18\n\"Headphones, Wireless\"\n3\n89.99\n269.97\n\n\n4\n\"Sarah\"\n\"Williams\"\n\"sarah.w@example.com\"\n2023-01-20\n\"Monitor\"\n1\n249.5\n249.5\n\n\n5\n\"Michael\"\n\"Brown\"\n\"michael.b@example.com\"\n2023-01-22\n\"Keyboard\"\n2\n59.99\n119.98\n\n\n\n\n\n\n\n\nJSON\nReading in JSON files is much the same The syntax is: pl.read_json(\"docs/data/path.json\")\nJSON files are a simpler file format to read in than CSVs and therfore have fewer parameters if you do want to read more check the documentation.\n\ndf_json = pl.read_json(\"basketball.json\")\n\ndf_json\n\n\nshape: (5, 6)\n\n\n\nplayer_name\nteam\nposition\npoints_per_game\nassists_per_game\nrebounds_per_game\n\n\nstr\nstr\nstr\nf64\nf64\nf64\n\n\n\n\n\"LeBron James\"\n\"Los Angeles Lakers\"\n\"Forward\"\n27.2\n7.4\n7.9\n\n\n\"Stephen Curry\"\n\"Golden State Warriors\"\n\"Guard\"\n29.5\n6.3\n5.2\n\n\n\"Giannis Antetokounmpo\"\n\"Milwaukee Bucks\"\n\"Forward\"\n31.1\n5.7\n11.8\n\n\n\"Kevin Durant\"\n\"Phoenix Suns\"\n\"Forward\"\n28.7\n5.3\n7.1\n\n\n\"Nikola Jokic\"\n\"Denver Nuggets\"\n\"Center\"\n24.5\n9.8\n11.3\n\n\n\n\n\n\n\n\nExcel\nPolars does not have a ‘native’ excel reader it instead utlilizes external librarys such as fastexcel, xlsx2csv, or openpyxl, to parse excel files into a format readable by polars. Of the three library listed above, polars reccomends using fastexcel. It is generally better to avoid using excel files whenever possible (which should be most of the time given you can export as a csv from excel) but it is obviously doable.\nbefore trying to read in the file ensure you have the requisite library installed. $ pip install fastexcel xlsx2csv openpyxl\nthe default syntax for reading in a xlsx file is as follows: pl.read_excel(\"path/to/data.xlsx\")\nif you have multiple sheets within the xlsx file you can specify which one to read in: df = pl.read_excel(\"path/to/data.xlsx\", sheet_name=\"example\")\nfor more here is the [documentation]\nhere is an exmple of reading a xlsx document:\n\ndf_xlsx = pl.read_excel(\"penguins.xlsx\", sheet_name=\"Dream Island\")\n\ndf_xlsx.tail(5)\n\n\nshape: (5, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\ni64\ni64\nstr\ni64\n\n\n\n\n\"Chinstrap\"\n\"Dream\"\n55.8\n19.8\n207\n4000\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n43.5\n18.1\n202\n3400\n\"female\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n49.6\n18.2\n193\n3775\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.8\n19.0\n210\n4100\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.2\n18.7\n198\n3775\n\"female\"\n2009\n\n\n\n\n\n\nthe spreadsheet used above can be found here"
  },
  {
    "objectID": "blog/posts/read_write_view/index.html#importing-data-with-polars",
    "href": "blog/posts/read_write_view/index.html#importing-data-with-polars",
    "title": "Introduction to Polars Pt. 2",
    "section": "Importing Data with Polars",
    "text": "Importing Data with Polars\n\n\nPolars provides robust capabilities for importing data from various sources including CSVs, JSONs, Excel spreadsheets, Parquet files, cloud storage solutions (AWS, Azure, and Google Cloud), and databases.\nThe importing methods follow a consistent pattern across file types, making it easy to work with different data formats.\n\n\n\n\n\nCSV Files\nThe basic syntax for reading a CSV file is:\npl.read_csv(\"path/to/data.csv\")\nAlternatively, you can also read CSV files directly from the internet:\npl.read_csv(\"https://example.com/path/to/your/file.csv\")\nThis capability to read files directly from URLs also works with all the file import methods we’ll cover below.\nThis function offers numerous parameters to handle different CSV formats and configurations. For more information read the documentation.\n\nimport polars as pl\n\ndf_csv = pl.read_csv(\"example.csv\", try_parse_dates=True)\ndf_csv.head(5)\n\n\nshape: (5, 9)\n\n\n\nid\nfirst_name\nlast_name\nemail\npurchase_date\nproduct\nquantity\nprice\ntotal\n\n\ni64\nstr\nstr\nstr\ndate\nstr\ni64\nf64\nf64\n\n\n\n\n1\n\"John\"\n\"Doe\"\n\"john.doe@example.com\"\n2023-01-15\n\"Laptop\"\n1\n1299.99\n1299.99\n\n\n2\n\"Jane\"\n\"Smith\"\n\"jane.smith@example.com\"\n2023-01-16\n\"Smartphone\"\n2\n699.95\n1399.9\n\n\n3\n\"Robert\"\n\"Johnson\"\n\"rob.j@example.com\"\n2023-01-18\n\"Headphones, Wireless\"\n3\n89.99\n269.97\n\n\n4\n\"Sarah\"\n\"Williams\"\n\"sarah.w@example.com\"\n2023-01-20\n\"Monitor\"\n1\n249.5\n249.5\n\n\n5\n\"Michael\"\n\"Brown\"\n\"michael.b@example.com\"\n2023-01-22\n\"Keyboard\"\n2\n59.99\n119.98\n\n\n\n\n\n\n\n\nJSON\n\n\nJSON Files\nReading JSON files follows a similar pattern. The basic syntax is:\npl.read_json(\"docs/data/path.json\")\nJSON files have a more standardized structure than CSVs, so the reading process requires fewer configuration parameters. Polars handles JSON parsing efficiently with minimal setup. For advanced options and configurations, consult the official documentation.\ndf_json = pl.read_json(\"basketball.json\")\n\ndf_json\n\n\nExcel\nPolars doesn’t include a native Excel reader. Instead, it leverages external libraries like fastexcel, xlsx2csv, or openpyxl to parse Excel files into Polars-compatible formats. Among these options, Polars recommends fastexcel for optimal performance.\nWhile it’s generally better to avoid using Excel files where possible (you can usually export as CSV directly from Excel), reading Excel files is straightforward with the right dependencies installed.\nBefore attempting to read Excel files, make sure you have at least one of these libraries installed:\n $ pip install fastexcel xlsx2csv openpyxl\nThe basic syntax for reading an Excel file with Polars is:\npl.read_excel(\"path/to/data.xlsx\")\nIf your Excel file contains multiple sheets, you can specify which one to read using the sheet_name parameter:\ndf = pl.read_excel(\"path/to/data.xlsx\", sheet_name=\"example\")\nFor additional Excel reading options and parameters, refer to the Polars Excel documentation, which covers sheet selection, range specification, and handling of complex Excel files.\n\ndf_xlsx = pl.read_excel(\"penguins.xlsx\", sheet_name=\"Dream Island\")\n\ndf_xlsx.tail(5)\n\n\nshape: (5, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\ni64\ni64\nstr\ni64\n\n\n\n\n\"Chinstrap\"\n\"Dream\"\n55.8\n19.8\n207\n4000\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n43.5\n18.1\n202\n3400\n\"female\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n49.6\n18.2\n193\n3775\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.8\n19.0\n210\n4100\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.2\n18.7\n198\n3775\n\"female\"\n2009\n\n\n\n\n\n\nThis example spreadsheet can be accessed via this Google Sheets link."
  },
  {
    "objectID": "blog/posts/read_and_write/index.html",
    "href": "blog/posts/read_and_write/index.html",
    "title": "Introduction to Polars Pt. 2",
    "section": "",
    "text": "In this segement, we’ll explore how to read and write data with Polars using various file formats including CSVs, JSONs, Excel spreadsheets, and Parquet files, whether stored locally or accessed via the web. Note that we won’t be covering cloud storage or database connections in this tutorial."
  },
  {
    "objectID": "blog/posts/read_and_write/index.html#importing-data-with-polars",
    "href": "blog/posts/read_and_write/index.html#importing-data-with-polars",
    "title": "Introduction to Polars Pt. 2",
    "section": "Importing Data with Polars",
    "text": "Importing Data with Polars\n\n\nPolars provides robust capabilities for importing data from various sources including CSVs, JSONs, Excel spreadsheets, Parquet files, cloud storage solutions (AWS, Azure, and Google Cloud), and databases.\nThe importing methods follow a consistent pattern across file types, making it easy to work with different data formats.\n\n\n\n\n\nCSV Files\nThe basic syntax for reading a CSV file is:\npl.read_csv(\"path/to/data.csv\")\nAlternatively, you can also read CSV files directly from the internet:\npl.read_csv(\"https://example.com/path/to/your/file.csv\")\nThis capability to read files directly from URLs also works with all the file import methods we’ll cover below.\nThis function offers numerous parameters to handle different CSV formats and configurations. For more information read the documentation.\n\nimport polars as pl\n\ndf_csv = pl.read_csv(\"example.csv\", try_parse_dates=True)\ndf_csv.head(5)\n\n\nshape: (5, 9)\n\n\n\nid\nfirst_name\nlast_name\nemail\npurchase_date\nproduct\nquantity\nprice\ntotal\n\n\ni64\nstr\nstr\nstr\ndate\nstr\ni64\nf64\nf64\n\n\n\n\n1\n\"John\"\n\"Doe\"\n\"john.doe@example.com\"\n2023-01-15\n\"Laptop\"\n1\n1299.99\n1299.99\n\n\n2\n\"Jane\"\n\"Smith\"\n\"jane.smith@example.com\"\n2023-01-16\n\"Smartphone\"\n2\n699.95\n1399.9\n\n\n3\n\"Robert\"\n\"Johnson\"\n\"rob.j@example.com\"\n2023-01-18\n\"Headphones, Wireless\"\n3\n89.99\n269.97\n\n\n4\n\"Sarah\"\n\"Williams\"\n\"sarah.w@example.com\"\n2023-01-20\n\"Monitor\"\n1\n249.5\n249.5\n\n\n5\n\"Michael\"\n\"Brown\"\n\"michael.b@example.com\"\n2023-01-22\n\"Keyboard\"\n2\n59.99\n119.98\n\n\n\n\n\n\n\n\nJSON\n\n\nJSON Files\nReading JSON files follows a similar pattern. The basic syntax is:\npl.read_json(\"docs/data/path.json\")\nJSON files have a more standardized structure than CSVs, so the reading process requires fewer configuration parameters. Polars handles JSON parsing efficiently with minimal setup. For advanced options and configurations, consult the official documentation.\ndf_json = pl.read_json(\"basketball.json\")\n\ndf_json\n\n\nExcel\nPolars doesn’t include a native Excel reader. Instead, it leverages external libraries like fastexcel, xlsx2csv, or openpyxl to parse Excel files into Polars-compatible formats. Among these options, Polars recommends fastexcel for optimal performance.\nWhile it’s generally better to avoid using Excel files where possible (you can usually export as CSV directly from Excel), reading Excel files is straightforward with the right dependencies installed.\nBefore attempting to read Excel files, make sure you have at least one of these libraries installed:\n $ pip install fastexcel xlsx2csv openpyxl\nThe basic syntax for reading an Excel file with Polars is:\npl.read_excel(\"path/to/data.xlsx\")\nIf your Excel file contains multiple sheets, you can specify which one to read using the sheet_name parameter:\ndf = pl.read_excel(\"path/to/data.xlsx\", sheet_name=\"example\")\nFor additional Excel reading options and parameters, refer to the Polars Excel documentation, which covers sheet selection, range specification, and handling of complex Excel files.\n\ndf_xlsx = pl.read_excel(\"penguins.xlsx\", sheet_name=\"Dream Island\")\n\ndf_xlsx.tail(5)\n\n\nshape: (5, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\ni64\ni64\nstr\ni64\n\n\n\n\n\"Chinstrap\"\n\"Dream\"\n55.8\n19.8\n207\n4000\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n43.5\n18.1\n202\n3400\n\"female\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n49.6\n18.2\n193\n3775\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.8\n19.0\n210\n4100\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.2\n18.7\n198\n3775\n\"female\"\n2009\n\n\n\n\n\n\nThis example spreadsheet can be accessed via this Google Sheets link.\n\n\nParquet\n\n\nImporting Mutiple files\n\n\nCombining files into dataframes"
  },
  {
    "objectID": "blog/posts/read_and_write/index.html#writing-data-with-polars",
    "href": "blog/posts/read_and_write/index.html#writing-data-with-polars",
    "title": "Introduction to Polars Pt. 2",
    "section": "Writing Data with Polars",
    "text": "Writing Data with Polars"
  },
  {
    "objectID": "blog/posts/read_data/index.html",
    "href": "blog/posts/read_data/index.html",
    "title": "Introduction to Polars Pt. 2",
    "section": "",
    "text": "In this segement, we’ll explore how to read data with Polars using various file formats including CSVs, JSONs, Excel spreadsheets, and Parquet files, whether stored locally or accessed via the web. Note that we won’t be covering cloud storage or database connections in this tutorial."
  },
  {
    "objectID": "blog/posts/read_data/index.html#importing-data-with-polars",
    "href": "blog/posts/read_data/index.html#importing-data-with-polars",
    "title": "Introduction to Polars Pt. 2",
    "section": "Importing Data with Polars",
    "text": "Importing Data with Polars\n\n\nPolars provides robust capabilities for importing data from various sources including CSVs, JSONs, Excel spreadsheets, Parquet files, cloud storage solutions (AWS, Azure, and Google Cloud), and databases.\nThe importing methods follow a consistent pattern across file types, making it easy to work with different data formats.\n\n\n\n\n\nCSV Files\nThe basic syntax for reading a CSV file is:\npl.read_csv(\"path/to/data.csv\")\nAlternatively, you can also read CSV files directly from the internet:\npl.read_csv(\"https://example.com/path/to/your/file.csv\")\nThis capability to read files directly from URLs also works with all the file import methods we’ll cover below.\nThis function offers numerous parameters to handle different CSV formats and configurations. For more information read the documentation.\n\nimport polars as pl\n\ndf_csv = pl.read_csv(\"example.csv\", try_parse_dates=True)\ndf_csv.head(5)\n\n\nshape: (5, 9)\n\n\n\nid\nfirst_name\nlast_name\nemail\npurchase_date\nproduct\nquantity\nprice\ntotal\n\n\ni64\nstr\nstr\nstr\ndate\nstr\ni64\nf64\nf64\n\n\n\n\n1\n\"John\"\n\"Doe\"\n\"john.doe@example.com\"\n2023-01-15\n\"Laptop\"\n1\n1299.99\n1299.99\n\n\n2\n\"Jane\"\n\"Smith\"\n\"jane.smith@example.com\"\n2023-01-16\n\"Smartphone\"\n2\n699.95\n1399.9\n\n\n3\n\"Robert\"\n\"Johnson\"\n\"rob.j@example.com\"\n2023-01-18\n\"Headphones, Wireless\"\n3\n89.99\n269.97\n\n\n4\n\"Sarah\"\n\"Williams\"\n\"sarah.w@example.com\"\n2023-01-20\n\"Monitor\"\n1\n249.5\n249.5\n\n\n5\n\"Michael\"\n\"Brown\"\n\"michael.b@example.com\"\n2023-01-22\n\"Keyboard\"\n2\n59.99\n119.98\n\n\n\n\n\n\n\n\nJSON Files\nReading JSON files follows a similar pattern. The basic syntax is:\npl.read_json(\"docs/data/path.json\")\nJSON files have a more standardized structure than CSVs, so the reading process requires fewer configuration parameters. Polars handles JSON parsing efficiently with minimal setup. For advanced options and configurations, consult the official documentation.\ndf_json = pl.read_json(\"basketball.json\")\n\ndf_json\n\n\nExcel\nPolars doesn’t include a native Excel reader. Instead, it leverages external libraries like fastexcel, xlsx2csv, or openpyxl to parse Excel files into Polars-compatible formats. Among these options, Polars recommends fastexcel for optimal performance.\nWhile it’s generally better to avoid using Excel files where possible (you can usually export as CSV directly from Excel), reading Excel files is straightforward with the right dependencies installed.\nBefore attempting to read Excel files, make sure you have at least one of these libraries installed:\n $ pip install fastexcel xlsx2csv openpyxl\nThe basic syntax for reading an Excel file with Polars is:\npl.read_excel(\"path/to/data.xlsx\")\nIf your Excel file contains multiple sheets, you can specify which one to read using the sheet_name parameter:\ndf = pl.read_excel(\"path/to/data.xlsx\", sheet_name=\"example\")\nFor additional Excel reading options and parameters, refer to the Polars Excel documentation, which covers sheet selection, range specification, and handling of complex Excel files.\n\ndf_xlsx = pl.read_excel(\"penguins.xlsx\", sheet_name=\"Dream Island\")\n\ndf_xlsx.tail(5)\n\n\nshape: (5, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\ni64\ni64\nstr\ni64\n\n\n\n\n\"Chinstrap\"\n\"Dream\"\n55.8\n19.8\n207\n4000\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n43.5\n18.1\n202\n3400\n\"female\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n49.6\n18.2\n193\n3775\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.8\n19.0\n210\n4100\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.2\n18.7\n198\n3775\n\"female\"\n2009\n\n\n\n\n\n\nThis example spreadsheet can be accessed via this Google Sheets link.\n\n\nParquet Files\nParquet is a columnar storage format designed for efficient data analytics. It provides excellent compression and fast query performance, making it a popular choice for data science workflows. Polars includes native, high-performance support for reading Parquet files.\nThe basic syntax for reading a Parquet file is:\npl.read_parquet(\"path/to/data.parquet\")\n\ndf_par = pl.read_parquet(\"finance.parquet\")\ndf_par.sample(4)\n\n\nshape: (4, 10)\n\n\n\ndate\nticker\nopen\nhigh\nlow\nclose\nvolume\npe_ratio\ndividend_yield\nmarket_cap\n\n\ndatetime[ns]\nstr\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\n\n\n\n\n2024-10-04 00:00:00\n\"AMZN\"\n156.15\n156.92\n155.65\n156.16\n2065500\n32.21\n0.0094\n6.3697e9\n\n\n2024-06-05 00:00:00\n\"AMZN\"\n129.89\n130.57\n129.4\n130.51\n2493218\n39.93\n0.0266\n5.2577e9\n\n\n2024-09-23 00:00:00\n\"META\"\n673.42\n677.51\n671.87\n674.46\n5167893\n22.97\n0.0294\n2.5092e10\n\n\n2025-01-31 00:00:00\n\"AMZN\"\n174.85\n176.84\n174.26\n175.72\n3196059\n33.99\n0.0104\n4.1035e9\n\n\n\n\n\n\n\n\nImporting Mutiple files\nFor situations where you need to combine data from multiple files into a single DataFrame, Polars offers straightforward approaches. While the syntax is relatively simple, the implementation may vary depending on your specific file organization.\nWhen working with multiple files of the same type and similar naming patterns in a single directory, Polars supports glob pattern matching:\npl.read_filetype(\"path/to/data/my_many_files_*.filetype\")\nFor files with different names but the same format, placing them in a single directory allows you to use wildcard patterns to import them all at once:\npl.read_filetype(\"path/to/data/import/*.filetype\")\nAlternatively, for files located in different directories or even on different servers, you can provide a list of filepaths or URLs:\npl.read_filetype([\n    \"path/to/first/file.filetype\",\n    \"path/to/second/file.filetype\",\n    \"another/location/file.filetype\"\n])\nIf you’re working with different file types that share the same schema (identical columns and datatypes) and want to combine them into a single DataFrame, you’ll need to read each file individually and then concatenate them. Polars makes this process straightforward with its concat function, which can merge DataFrames regardless of their original file formats.\n# Read files of different formats\ndf1 = pl.read_csv(\"path/to/file.csv\")\ndf2 = pl.read_parquet(\"path/to/file.parquet\")\ndf3 = pl.read_json(\"path/to/file.json\")\n\n# Concatenate into a single DataFrame\ncombined_df = pl.concat([df1, df2, df3], how=\"vertical\")"
  },
  {
    "objectID": "intro_to_polars/chapter4.html",
    "href": "intro_to_polars/chapter4.html",
    "title": "Clark",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "intro_to_polars/appendix2.html",
    "href": "intro_to_polars/appendix2.html",
    "title": "Reading and Writing Data",
    "section": "",
    "text": "Reading and Writing Data\n\n\n\n\n Back to top"
  },
  {
    "objectID": "intro_to_polars/chapter3.html",
    "href": "intro_to_polars/chapter3.html",
    "title": "Dataframes and Series",
    "section": "",
    "text": "Polars allows you to store data in a variety of formats called data types. These data types fall generally into the following categories:\n\nNumeric: Signed integers, unsigned integers, floating point numbers, and decimals\nNested: Lists, structs, and arrays for handling complex data\nTemporal: Dates, datetimes, and times for working with time-based data\nMiscellaneous: Strings, binary data, Booleans, categoricals, enums, and objects\n\nThe most common data types you will be working with are generally: Strings, signed and unsigned integers, floating point numbers or floats, decimals, dates or datetimes and booleans. For more information on each of these data types see ?@sec-appendix-a."
  },
  {
    "objectID": "intro_to_polars/chapter3.html#data-types",
    "href": "intro_to_polars/chapter3.html#data-types",
    "title": "Dataframes and Series",
    "section": "",
    "text": "Polars allows you to store data in a variety of formats called data types. These data types fall generally into the following categories:\n\nNumeric: Signed integers, unsigned integers, floating point numbers, and decimals\nNested: Lists, structs, and arrays for handling complex data\nTemporal: Dates, datetimes, and times for working with time-based data\nMiscellaneous: Strings, binary data, Booleans, categoricals, enums, and objects\n\nThe most common data types you will be working with are generally: Strings, signed and unsigned integers, floating point numbers or floats, decimals, dates or datetimes and booleans. For more information on each of these data types see ?@sec-appendix-a."
  },
  {
    "objectID": "intro_to_polars/chapter3.html#series",
    "href": "intro_to_polars/chapter3.html#series",
    "title": "Dataframes and Series",
    "section": "Series",
    "text": "Series\nThe two most common data structures in Polars are DataFrames and Series. Series are one-dimensional data structures where\nCreating a Series is straightforward with the following syntax:\npl.Series(name, values_list)\nWhere “name” is the label for your Series and “values_list” contains the data. Here’s a simple example:\n\nimport polars as pl\ns = pl.Series(\"example\", [1, 2, 3, 4, 5])\ns\n\n\nshape: (5,)\n\n\n\nexample\n\n\ni64\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\nWhen you create a series Polars will infer the data type for the values you provide. So in the above example I gave it [1, 2, 3, 4, 5] and it set the datatype to Int64 if instead gave it [1, 2, 3, 4.0, 5] it would asume it is Float64.\n\ns2 = pl.Series(\"payment\", [132.50, 120, 116, 98.75 ,42])\ns2\n\n\nshape: (5,)\n\n\n\npayment\n\n\nf64\n\n\n\n\n132.5\n\n\n120.0\n\n\n116.0\n\n\n98.75\n\n\n42.0\n\n\n\n\n\n\n\ns3 = pl.Series(\"mixed\", [1, \"text\", True, 3.14], strict=False)\n# series.dytpe outputs a the data type of the series\nprint(f\"Mixed series type: {s3.dtype}\")\ns3\n\nMixed series type: String\n\n\n\nshape: (4,)\n\n\n\nmixed\n\n\nstr\n\n\n\n\n\"1\"\n\n\n\"text\"\n\n\n\"true\"\n\n\n\"3.14\"\n\n\n\n\n\n\nYou can set the data type of the series as well by using the dtype parameter. A example use case is when storing a id number the id number should be stored as a string not a int due to the fact that we we do not want to perform mathmatical operations on the identification number therefore it is best stored as a string.\n\n# strict=False allows automatic conversion from different data types\ns3 = pl.Series(\"id number\", [143823, 194203, 553420, 234325, 236532], dtype=pl.Utf8, strict=False)\ns3\n\n\nshape: (5,)\n\n\n\nid number\n\n\nstr\n\n\n\n\n\"143823\"\n\n\n\"194203\"\n\n\n\"553420\"\n\n\n\"234325\"\n\n\n\"236532\""
  },
  {
    "objectID": "intro_to_polars/chapter3.html#dataframes",
    "href": "intro_to_polars/chapter3.html#dataframes",
    "title": "Dataframes and Series",
    "section": "Dataframes",
    "text": "Dataframes\nDataFrames are tabular data structures (rows and columns) composed of multiple Series, with each column representing a single Series. The design of a dataframe is called schema. A schema is a mapping of column to the data types.\nDataframes are the workhorses of data analysis and what you’ll use most frequently.\nWith DataFrames, you can write powerful queries to filter, transform, aggregate, and reshape your data efficiently.\nDataFrames can be created in several ways:\n\nFrom a dictionary of sequences (lists, arrays)\nWith explicit schema specification\nFrom a sequence of (name, dtype) pairs\nFrom NumPy arrays\nFrom a list of lists (row-oriented data)\nBy converting pandas DataFrames\nBy importing existing tabular data from CSVs, JSON, SQL, Parquet files, etc.\n\nIn real-world environments, you’ll typically work with preexisting data, though understanding various creation methods is valuable. We’ll cover data import techniques later, but for now, here’s an example of a DataFrame created from a dictionary of lists:\n\n# Create a DataFrame from a dictionary of lists\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    \"age\": [25, 30, 35, 40],\n    \"city\": [\"New York\", \"Boston\", \"Chicago\", \"Seattle\"],\n    \"salary\": [75000, 85000, 90000, 95000]\n})\n\ndf\n\n\nshape: (4, 4)\n\n\n\nname\nage\ncity\nsalary\n\n\nstr\ni64\nstr\ni64\n\n\n\n\n\"Alice\"\n25\n\"New York\"\n75000\n\n\n\"Bob\"\n30\n\"Boston\"\n85000\n\n\n\"Charlie\"\n35\n\"Chicago\"\n90000\n\n\n\"David\"\n40\n\"Seattle\"\n95000\n\n\n\n\n\n\nevery data frame has a shape. the shape is the number of rows and columns in a dataframe shape(rows,columns)\nthe shape for the above dataframe is:\n\nprint(df.shape)\n\n(4, 4)\n\n\nyou can view the schema of any dataframe with the following command\n\nprint(df.schema)\n\nSchema({'name': String, 'age': Int64, 'city': String, 'salary': Int64})\n\n\nWe see here that the schema is returned as a dictionary. In the above example the column name has the string datatype. Though you can view the data type already when displaying the dataframe."
  },
  {
    "objectID": "intro_to_polars/chapter3.html#inspecting-dataframes",
    "href": "intro_to_polars/chapter3.html#inspecting-dataframes",
    "title": "Dataframes and Series",
    "section": "Inspecting Dataframes",
    "text": "Inspecting Dataframes\nIn polars there are a varity of ways to inspect a dataframe, all of which have different use cases. The ones that we will be covering right now are:\n\nhead\ntail\nglimpse\nsample\ndescribe\nslice\n\n\nhead\nthe head functions allows you to view the first x rows of the dataframe. By default the number of rows it shows is 5, though you can specify the number of rows to view.\ndataframe.head(n)\nWhere n is the number of rows to return if you give it a negative number it will turn all rows except the last n rows.\n\nimport numpy as np\n\n# Create NumPy arrays for sandwich data\nsandwich_names = np.array(['BLT', 'Club', 'Tuna', 'Ham & Cheese', 'Veggie'])\nprices = np.array([8.99, 10.50, 7.50, 6.99, 6.50])\ncalories = np.array([550, 720, 480, 520, 320])\nvegetarian = np.array([False, False, False, False, True])\n\n# Create DataFrame from NumPy arrays\nsandwich_df = pl.DataFrame({\n    \"sandwich\": sandwich_names,\n    \"price\": prices,\n    \"calories\": calories,\n    \"vegetarian\": vegetarian\n})\n\n\nsandwich_df.head(3)\n\n\nshape: (3, 4)\n\n\n\nsandwich\nprice\ncalories\nvegetarian\n\n\nstr\nf64\ni64\nbool\n\n\n\n\n\"BLT\"\n8.99\n550\nfalse\n\n\n\"Club\"\n10.5\n720\nfalse\n\n\n\"Tuna\"\n7.5\n480\nfalse\n\n\n\n\n\n\n\n\ntail\nThe tail function is essentially the inverse of head. It allows you to view the last n rows of the dataframe. The default for tail is also five rows.\ndataframe.tail(n)\nWhere n is the number of rows to return if you give it a negative number it will turn all rows except the first n rows."
  },
  {
    "objectID": "intro_to_polars/index.html",
    "href": "intro_to_polars/index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nWelcome to “Introduction to Polars.” This book emerged from my journey as a data science student who chose to explore Polars rather than pandas—the standard library taught in my course. When I approached my instructor about using this alternative technology, he supported my decision while honestly acknowledging that course materials wouldn’t cover my chosen path.\nAs I navigated through the course, I discovered a significant gap in beginner-friendly Polars resources for data science newcomers. While the official documentation proved valuable, it often assumed a level of familiarity that beginners might not possess. Nevertheless, through persistence and experimentation, I gained proficiency and successfully completed my coursework.\nThis book aims to bridge that gap by offering an accessible introduction to Polars for those new to data manipulation libraries. I’ve designed it especially for readers with limited prior experience in data science, incorporating the insights and solutions I discovered along my learning journey.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Visualization/metal_in_mistborn.html",
    "href": "Visualization/metal_in_mistborn.html",
    "title": "Metals in Mistborn",
    "section": "",
    "text": "I’ve loved reading since I was little, and that passion has only grown stronger over the years. There’s little I enjoy more than curling up on the couch or in bed with a good book, completely absorbed in another world.\nRecently, I decided to merge two of my favorite things: data science and reading. I wanted to create visualizations of some of my favorite books, though I wasn’t sure initially how to approach it or which tools to use. Through research, I discovered tidytext, a textual analysis package in R that opened up exciting possibilities for exploration and discovery.\nWhat you’ll see below represents the first fruits of this labor of love, a journey that combines the analytical precision of data science with the narrative richness of literature."
  },
  {
    "objectID": "Visualization/metal_in_mistborn.html#introduction",
    "href": "Visualization/metal_in_mistborn.html#introduction",
    "title": "Metals in Mistborn",
    "section": "",
    "text": "I’ve loved reading since I was little, and that passion has only grown stronger over the years. There’s little I enjoy more than curling up on the couch or in bed with a good book, completely absorbed in another world.\nRecently, I decided to merge two of my favorite things: data science and reading. I wanted to create visualizations of some of my favorite books, though I wasn’t sure initially how to approach it or which tools to use. Through research, I discovered tidytext, a textual analysis package in R that opened up exciting possibilities for exploration and discovery.\nWhat you’ll see below represents the first fruits of this labor of love, a journey that combines the analytical precision of data science with the narrative richness of literature."
  },
  {
    "objectID": "Visualization/metal_in_mistborn.html#tools-and-libraries",
    "href": "Visualization/metal_in_mistborn.html#tools-and-libraries",
    "title": "Metals in Mistborn",
    "section": "Tools and Libraries",
    "text": "Tools and Libraries\nThe project utilizes several R libraries:\n\ntidyverse: For data manipulation and visualization.\ntidytext: For text processing and tokenization.\npdftools: To extract text from the PDF.\nggstream, ggraph, igraph: For creating stream graphs and network visualizations.\ngt: For styling tables.\nwidyr: For calculating word correlations.\n\nIn addition canva was also used for formatting the final product."
  },
  {
    "objectID": "Visualization/metal_in_mistborn.html#data-extraction-and-cleaning",
    "href": "Visualization/metal_in_mistborn.html#data-extraction-and-cleaning",
    "title": "Metals in Mistborn",
    "section": "Data Extraction and Cleaning",
    "text": "Data Extraction and Cleaning\nThe first challenge is getting the text out of the PDF file and preparing it for analysis.\n\nReading the PDF\nWe begin by loading the PDF using pdftools:\n\nmistborn_text &lt;- pdf_text(\"./final_empire.pdf\")\n\nThis creates a character vector where each element is a page of the book.\n\n\nCleaning the Text\nThe raw text includes unwanted elements like page headers (e.g., “mistborn 123”) and footers (e.g., “123 brandon sanderson”). We split the text into lines and clean it with regular expressions:\n\ntext_lines &lt;- unlist(strsplit(mistborn_text, \"\\n\")) %&gt;%\n  str_replace_all(\"mistborn\\\\s+\\\\d+\", \"\") %&gt;%\n  str_replace_all(\"\\\\d+\\\\s+brandon sanderson\", \"\") %&gt;%\n  str_replace_all(\"\\n\", \" \") %&gt;%\n  str_replace_all(\"[^[:alnum:][:space:]\\\\.]\", \" \")\n\nThis removes headers, footers, and special characters, leaving only alphanumeric text, spaces, and periods.\n\n\nExtracting the Story\nThe actual story starts at “PROLOGUE” and ends before “ARS ARCANUM.” We locate these markers and extract the relevant lines:\n\nprologue_line &lt;- which(str_detect(text_lines, \"PROLOGUE$\"))\nars_arcanum_line &lt;- which(str_detect(text_lines, \"ARS ARCANUM$\"))\nstory_lines &lt;- text_lines[prologue_line:(ars_arcanum_line - 1)]\n\n\n\nIdentifying Chapters\nChapters are marked by lines containing “PROLOGUE,” “EPILOGUE,” or a single number (e.g., “1”). We find these markers and assign each line to a chapter:\n\nchapter_markers &lt;- which(str_detect(story_lines, \"\\\\b\\\\d{1,2}\\\\b\") |\n                         str_detect(story_lines, \"PROLOGUE$\") |\n                         str_detect(story_lines, \"EPILOGUE$\"))\nchapter_names &lt;- story_lines[chapter_markers]\nline_chapters &lt;- findInterval(seq_along(story_lines), chapter_markers)\nchapter_labels &lt;- chapter_names[line_chapters]\n\nWe also create a tibble mapping chapter names to unique IDs:\n\nchapter_id &lt;- tibble(\n  chapter_name = str_trim(chapter_names),\n  chapter_id = seq(1, 40)\n)\nhead(chapter_id,4) %&gt;% kable()\n\n\n\n\nchapter_name\nchapter_id\n\n\n\n\nPROLOGUE\n1\n\n\n1\n2\n\n\n2\n3\n\n\n3\n4\n\n\n\n\n\n\n\nStructuring the Data\nThe cleaned text is organized into a tibble with columns for chapter name, text, line number, and chapter ID:\n\ndf &lt;- tibble(\n  chapter = str_trim(chapter_labels),\n  text = story_lines\n) %&gt;%\n  group_by(chapter) %&gt;%\n  mutate(line_number = row_number()) %&gt;%\n  ungroup() %&gt;%\n  inner_join(chapter_id, join_by(chapter == chapter_name))\n\nhead(df,4) %&gt;% kable()\n\n\n\n\n\n\n\n\n\n\nchapter\ntext\nline_number\nchapter_id\n\n\n\n\nPROLOGUE\nPROLOGUE\n1\n1\n\n\nPROLOGUE\nASH FELL FROM THE SKY.\n2\n1\n\n\nPROLOGUE\nLord Tresting frowned glancing up at the ruddy midday\n3\n1\n\n\nPROLOGUE\nsky as his servants scuttled forward opening a parasol over\n4\n1\n\n\n\n\n\nWe also extract the book’s parts (e.g., “PART ONE”) for use in visualizations:\n\npart_lines &lt;- df %&gt;%\n  filter(str_detect(text, \"^\\\\s+PART\")) %&gt;%\n  mutate(text = str_trim(text)) %&gt;%\n  rename(part = text) %&gt;%\n  select(chapter_id, part)\n\npart_lines %&gt;% kable()\n\n\n\n\nchapter_id\npart\n\n\n\n\n1\nPART ONE\n\n\n9\nPART TWO\n\n\n16\nPART THREE\n\n\n26\nPART FOUR\n\n\n35\nPART FIVE"
  },
  {
    "objectID": "Visualization/metal_in_mistborn.html#text-analysis",
    "href": "Visualization/metal_in_mistborn.html#text-analysis",
    "title": "Metals in Mistborn",
    "section": "Text Analysis",
    "text": "Text Analysis\nWith the text structured, we analyze mentions of the Allomantic metals: iron, steel, tin, pewter, brass, zinc, copper, bronze, atium, and gold.\n\nTokenizing the Text\nUsing tidytext, we break the text into individual words:\n\ntext_df &lt;- df %&gt;%\n  unnest_tokens(word, text)\n\nhead(text_df) %&gt;% kable()\n\n\n\n\nchapter\nline_number\nchapter_id\nword\n\n\n\n\nPROLOGUE\n1\n1\nprologue\n\n\nPROLOGUE\n2\n1\nash\n\n\nPROLOGUE\n2\n1\nfell\n\n\nPROLOGUE\n2\n1\nfrom\n\n\nPROLOGUE\n2\n1\nthe\n\n\nPROLOGUE\n2\n1\nsky\n\n\n\n\n\n\n\nCounting Metal Mentions\nWe filter for the metals and count their mentions per chapter:\n\nmetals_list &lt;- c(\"iron\", \"steel\", \"tin\", \"pewter\", \"brass\", \"zinc\", \"copper\", \"bronze\", \"atium\", \"gold\")\nmetals_text &lt;- text_df %&gt;%\n  filter(word %in% metals_list) %&gt;%\n  mutate(word = str_to_title(word)) %&gt;%\n  count(chapter_id, word, name = \"mentions\") %&gt;%\n  left_join(part_lines, join_by(chapter_id == chapter_id)) %&gt;%\n  fill(part)\n\nhead(metals_text,4) %&gt;% kable()\n\n\n\n\nchapter_id\nword\nmentions\npart\n\n\n\n\n1\nIron\n1\nPART ONE\n\n\n1\nSteel\n2\nPART ONE\n\n\n1\nTin\n5\nPART ONE\n\n\n2\nGold\n1\nPART ONE\n\n\n\n\n\nThis produces a dataset with each metal’s mention count per chapter, linked to the book’s parts.\n\n\nStream graph\nThe stream graph was generated from the data above.\n\nstream_graph &lt;- ggplot() +\n  # Add part divider lines\n  geom_vline(\n    data = part_lines,\n    aes(xintercept = chapter_id),\n    color = \"grey50\",\n    linewidth = 0.2,\n    linetype = 2\n  ) +\n  # Streamgraph layer\n  geom_stream(\n    data = metals_text,\n    aes(x = chapter_id, y = mentions, fill = word),\n    type = \"mirror\",\n    extra_span = 0.1,\n    bw = 0.55\n  ) +\n  # Add part labels at the bottom of the plot\n  geom_text(\n    data = metals_text %&gt;%\n      group_by(part) %&gt;%\n      summarize(x = min(chapter_id) + n_distinct(chapter_id) / 2),\n    aes(x, y = -Inf, label = part),\n    vjust = -1,\n    hjust = 0.5,\n    color = \"grey60\",\n    family = \"Trajan Pro\",\n    size = 5\n  ) +\n  # Custom fill scale for metals\n  scale_fill_manual(values = metal_colors) +\n  labs(\n    fill = \"Metal\",\n    subtitle = \"Distribution of mentions share (number of words)&lt;br&gt;\n    per metal in each chapter. Parts are separated with vertical lines.\"\n  ) +\n  # Apply custom theme\n  theme_mistborn()\n\nstream_graph\n\n\n\n\n\n\n\n\nEach metal is a colored stream, with vertical lines marking the book’s parts. This highlights shifts in focus, like a spike in “atium” mentions during key plot moments.\n\n\nBar Plot\nA bar plot displays total mentions for each metal:\n\n# Summarize metal mention counts and reorder factors\nplot_data &lt;- metals_text %&gt;% \n  group_by(word) %&gt;% \n  summarise(total_mentions = sum(mentions)) %&gt;% \n  arrange(desc(total_mentions)) %&gt;% \n  mutate(word = fct_reorder(word, total_mentions))\n\n# Create horizontal bar plot\nbar_plot &lt;- plot_data %&gt;% \n  ggplot(aes(x = word, y = total_mentions, fill = word)) +\n  # Bar geometry\n  geom_col() +\n  # Custom fill colors\n  scale_fill_manual(values = metal_colors) +\n  # Remove legend\n  guides(fill = \"none\") +\n  # Labels\n  labs(y = \"Number of Mentions\") +\n  # Allow labels to extend past plot area\n  coord_cartesian(clip = \"off\") +\n  # Text labels above bars\n  geom_text(\n    aes(label = total_mentions), \n    color = \"grey70\",\n    hjust = 0.5,\n    vjust = -0.4,\n    size = 7.5,\n    family = \"Trajan Pro\"\n  ) +\n  # Apply custom theme\n  theme_mist() +\n  # Additional theme tweaks\n  theme(\n    axis.text.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_text(size = 18, angle = 90)\n  )\n\nbar_plot\n\n\n\n\n\n\n\n\n\n\nExploring Word Correlations\nTo see how metals are used in context, we calculate correlations between words appearing together. We flatten the text, split it into sentences, group sentences into sections of ten, and compute pairwise correlations for words appearing at least 20 times:\n\nmist_section &lt;- str_flatten(story_lines, collapse = \" \") %&gt;%\n  as_tibble() %&gt;%\n  unnest_sentences(sentence, value) %&gt;%\n  mutate(section = row_number() %/% 10) %&gt;%\n  unnest_tokens(word, sentence) %&gt;%\n  filter(!word %in% stop_words$word)\n\nword_cors &lt;- mist_section %&gt;%\n  group_by(word) %&gt;%\n  filter(n() &gt;= 20) %&gt;%\n  pairwise_cor(word, section, sort = TRUE) %&gt;%\n  filter(item1 %in% metals_list)\n\nhead(word_cors,4) %&gt;% kable()\n\n\n\n\nitem1\nitem2\ncorrelation\n\n\n\n\ntin\nenhanced\n0.3414959\n\n\npewter\nﬂared\n0.3408370\n\n\ncopper\nburning\n0.3076759\n\n\niron\nsteel\n0.2836302\n\n\n\n\n\nThis reveals which words are frequently associated with each metal.\n\n\nNetwork Graph\n\n# Set seed for reproducible layout\nset.seed(2000)\n\n# Filter word correlations and create graph\ngraph &lt;- word_cors %&gt;% \n  filter(correlation &gt; 0.15) %&gt;%\n  graph_from_data_frame()\n\n# Compute node degrees and tag metals\nV(graph)$degree &lt;- degree(graph)\nV(graph)$is_metal &lt;- V(graph)$name %in% metals_list  # Boolean for coloring nodes\n\n# Create network graph with conditional styling\nnetwork_Graph &lt;- graph %&gt;%\n  ggraph(layout = \"fr\") +\n  # Draw edges with thickness and transparency based on correlation\n  geom_edge_link(\n    aes(edge_alpha = correlation, edge_width = correlation), \n    color = \"grey40\",\n    show.legend = FALSE\n  ) +\n  # Draw nodes with size by degree and color by is_metal\n  geom_node_point(\n    aes(size = degree, color = is_metal)\n  ) +\n  # Custom colors for metal vs non-metal nodes\n  scale_color_manual(\n    values = c(\"FALSE\" = \"#ff79c6\", \"TRUE\" = \"lightblue\")\n  ) +\n  # Add labels with soft background\n  geom_node_label(\n    aes(label = name),\n    color = \"grey95\",\n    fill = \"#4D267380\",\n    repel = TRUE,\n    force = 1,\n    force_pull = 0.1,\n    family = \"Trajan Pro\",\n    label.size = 0,\n    size = 3.5\n  ) +\n  # Control node size range\n  scale_size_continuous(range = c(3, 10)) +\n  # Remove legends\n  guides(size = \"none\", color = \"none\") +\n  # Apply custom theme\n  theme_mistborn()\n\nnetwork_Graph\n\n\n\n\n\n\n\n\nMetals (in light blue) connect to related terms (in pink), showing context—like “steel” linking to “push.”\nDue to a desire for brevity I left some of the code out, to see the entire code you can view it on my github."
  },
  {
    "objectID": "Visualization/metal_in_mistborn.html#visualizations",
    "href": "Visualization/metal_in_mistborn.html#visualizations",
    "title": "Metals in Mistborn",
    "section": "Visualizations",
    "text": "Visualizations\nThe project creates four visualizations to showcase the findings.\n\nNetwork Graph\nThe network graph visualizes word correlations:\n\n# Set seed for reproducible layout\nset.seed(2000)\n\n# Filter word correlations and create graph\ngraph &lt;- word_cors %&gt;% \n  filter(correlation &gt; 0.15) %&gt;%\n  graph_from_data_frame()\n\n# Compute node degrees and tag metals\nV(graph)$degree &lt;- degree(graph)\nV(graph)$is_metal &lt;- V(graph)$name %in% metals_list  # Boolean for coloring nodes\n\n# Create network graph with conditional styling\nnetwork_Graph &lt;- graph %&gt;%\n  ggraph(layout = \"fr\") +\n  # Draw edges with thickness and transparency based on correlation\n  geom_edge_link(\n    aes(edge_alpha = correlation, edge_width = correlation), \n    color = \"grey40\",\n    show.legend = FALSE\n  ) +\n  # Draw nodes with size by degree and color by is_metal\n  geom_node_point(\n    aes(size = degree, color = is_metal)\n  ) +\n  # Custom colors for metal vs non-metal nodes\n  scale_color_manual(\n    values = c(\"FALSE\" = \"#ff79c6\", \"TRUE\" = \"lightblue\")\n  ) +\n  # Add labels with soft background\n  geom_node_label(\n    aes(label = name),\n    color = \"grey95\",\n    fill = \"#4D267380\",\n    repel = TRUE,\n    force = 1,\n    force_pull = 0.1,\n    family = \"Trajan Pro\",\n    label.size = 0,\n    size = 3.5\n  ) +\n  # Control node size range\n  scale_size_continuous(range = c(3, 10)) +\n  # Remove legends\n  guides(size = \"none\", color = \"none\") +\n  # Apply custom theme\n  theme_mistborn()\n\nnetwork_Graph\n\n\n\n\n\n\n\n\nMetals (in light blue) connect to related terms (in pink), showing context—like “steel” linking to “push.”"
  },
  {
    "objectID": "Visualization/metal_in_mistborn.html#insights-and-conclusion",
    "href": "Visualization/metal_in_mistborn.html#insights-and-conclusion",
    "title": "Metals in Mistborn",
    "section": "Insights and Conclusion",
    "text": "Insights and Conclusion\nThe analysis reveals that pewter and steel dominate mentions, aligning with their frequent use by key characters. The stream graph shows narrative shifts, like atium’s prominence in the climax. The network graph adds depth, linking metals to actions and concepts in the story.\nThis project illustrates how data science can enrich literary analysis, offering a fresh lens on Mistborn. The code and methods are adaptable to other texts, inviting further exploration at the intersection of data and storytelling."
  },
  {
    "objectID": "Visualization/metal_in_mistborn.html#conclusion",
    "href": "Visualization/metal_in_mistborn.html#conclusion",
    "title": "Metals in Mistborn",
    "section": "Conclusion",
    "text": "Conclusion\nThroughout this project, I gained valuable experience and enjoyed the journey of exploration and experimentation. I explored numerous approaches and visualization options before settling on my final choices, and I’m quite pleased with how everything turned out."
  },
  {
    "objectID": "Visualization/metal_in_mistborn.html#poster",
    "href": "Visualization/metal_in_mistborn.html#poster",
    "title": "Metals in Mistborn",
    "section": "Poster",
    "text": "Poster\nThe final image was creating using canva to format and style it."
  },
  {
    "objectID": "Projects/metal_in_mistborn.html",
    "href": "Projects/metal_in_mistborn.html",
    "title": "Metals in Mistborn",
    "section": "",
    "text": "I’ve always loved reading. Recently, I decided to combine this passion with data science by creating visualizations of my favorite books. After some research, I found tidytext, an R package for analyzing text, which opened up new ways to explore literature through data."
  },
  {
    "objectID": "Projects/metal_in_mistborn.html#introduction",
    "href": "Projects/metal_in_mistborn.html#introduction",
    "title": "Metals in Mistborn",
    "section": "",
    "text": "I’ve always loved reading. Recently, I decided to combine this passion with data science by creating visualizations of my favorite books. After some research, I found tidytext, an R package for analyzing text, which opened up new ways to explore literature through data."
  },
  {
    "objectID": "Projects/metal_in_mistborn.html#analyzing-metals-in-mistborn",
    "href": "Projects/metal_in_mistborn.html#analyzing-metals-in-mistborn",
    "title": "Metals in Mistborn",
    "section": "Analyzing Metals in Mistborn",
    "text": "Analyzing Metals in Mistborn\nOne of my favorite books is Brandon Sanderson’s Mistborn: The Final Empire. In this fantasy novel, metals are the source of magical powers through a system called Allomancy—people can “burn” different metals to gain special abilities. I used R and text analysis tools to track how these metals appear throughout the story. By processing the entire book and turning the text into data, I created visualizations that reveal patterns you might not notice while reading.\nWhat follows is both a technical walkthrough and a love letter to literature."
  },
  {
    "objectID": "Projects/metal_in_mistborn.html#tools-and-libraries",
    "href": "Projects/metal_in_mistborn.html#tools-and-libraries",
    "title": "Metals in Mistborn",
    "section": "Tools and Libraries",
    "text": "Tools and Libraries\n\n\nImporting fonts may take a few minutes, depending on the number of fonts and the speed of the system.\nContinue? [y/n] \n\n\nImporting fonts may take a few minutes, depending on the number of fonts and the speed of the system.\nContinue? [y/n] \n\n\nThe project utilizes several R libraries:\n\ntidyverse: For data manipulation and visualization.\ntidytext: For text processing and tokenization.\npdftools: To extract text from the PDF.\nggstream, ggraph, igraph: For creating stream graphs and network visualizations.\ngt: For styling tables.\nwidyr: For calculating word correlations.\n\nIn addition canva was also used for formatting the final product."
  },
  {
    "objectID": "Projects/metal_in_mistborn.html#data-extraction-and-cleaning",
    "href": "Projects/metal_in_mistborn.html#data-extraction-and-cleaning",
    "title": "Metals in Mistborn",
    "section": "Data Extraction and Cleaning",
    "text": "Data Extraction and Cleaning\nThe first challenge is getting the text out of the PDF file and preparing it for analysis.\n\nReading the PDF\nWe begin by loading the PDF using pdftools:\n\nmistborn_text &lt;- pdf_text(\"./final_empire.pdf\")\n\nThis creates a character vector where each element is a page of the book.\n\n\nCleaning the Text\nThe raw text includes unwanted elements like page headers (e.g., “mistborn 123”) and footers (e.g., “123 brandon sanderson”). We split the text into lines and clean it with regular expressions:\n\ntext_lines &lt;- unlist(strsplit(mistborn_text, \"\\n\")) %&gt;%\n  str_replace_all(\"mistborn\\\\s+\\\\d+\", \"\") %&gt;%\n  str_replace_all(\"\\\\d+\\\\s+brandon sanderson\", \"\") %&gt;%\n  str_replace_all(\"\\n\", \" \") %&gt;%\n  str_replace_all(\"[^[:alnum:][:space:]\\\\.]\", \" \")\n\nThis removes headers, footers, and special characters, leaving only alphanumeric text, spaces, and periods.\n\n\nExtracting the Story\nThe actual story starts at “PROLOGUE” and ends before “ARS ARCANUM.” We locate these markers and extract the relevant lines:\n\nprologue_line &lt;- which(str_detect(text_lines, \"PROLOGUE$\"))\nars_arcanum_line &lt;- which(str_detect(text_lines, \"ARS ARCANUM$\"))\nstory_lines &lt;- text_lines[prologue_line:(ars_arcanum_line - 1)]\n\n\n\nIdentifying Chapters\nChapters are marked by lines containing “PROLOGUE,” “EPILOGUE,” or a single number (e.g., “1”). We find these markers and assign each line to a chapter:\n\nchapter_markers &lt;- which(str_detect(story_lines, \"\\\\b\\\\d{1,2}\\\\b\") |\n                         str_detect(story_lines, \"PROLOGUE$\") |\n                         str_detect(story_lines, \"EPILOGUE$\"))\nchapter_names &lt;- story_lines[chapter_markers]\nline_chapters &lt;- findInterval(seq_along(story_lines), chapter_markers)\nchapter_labels &lt;- chapter_names[line_chapters]\n\nWe also create a tibble mapping chapter names to unique IDs:\n\nchapter_id &lt;- tibble(\n  chapter_name = str_trim(chapter_names),\n  chapter_id = seq(1, 40)\n)\nhead(chapter_id,4) %&gt;% kable()\n\n\n\n\nchapter_name\nchapter_id\n\n\n\n\nPROLOGUE\n1\n\n\n1\n2\n\n\n2\n3\n\n\n3\n4\n\n\n\n\n\n\n\nStructuring the Data\nThe cleaned text is organized into a tibble with columns for chapter name, text, line number, and chapter ID:\n\ndf &lt;- tibble(\n  chapter = str_trim(chapter_labels),\n  text = story_lines\n) %&gt;%\n  group_by(chapter) %&gt;%\n  mutate(line_number = row_number()) %&gt;%\n  ungroup() %&gt;%\n  inner_join(chapter_id, join_by(chapter == chapter_name))\n\nhead(df,4) %&gt;% kable()\n\n\n\n\n\n\n\n\n\n\nchapter\ntext\nline_number\nchapter_id\n\n\n\n\nPROLOGUE\nPROLOGUE\n1\n1\n\n\nPROLOGUE\nASH FELL FROM THE SKY.\n2\n1\n\n\nPROLOGUE\nLord Tresting frowned glancing up at the ruddy midday\n3\n1\n\n\nPROLOGUE\nsky as his servants scuttled forward opening a parasol over\n4\n1\n\n\n\n\n\nWe also extract the book’s parts (e.g., “PART ONE”) for use in visualizations:\n\npart_lines &lt;- df %&gt;%\n  filter(str_detect(text, \"^\\\\s+PART\")) %&gt;%\n  mutate(text = str_trim(text)) %&gt;%\n  rename(part = text) %&gt;%\n  select(chapter_id, part)\n\npart_lines %&gt;% kable()\n\n\n\n\nchapter_id\npart\n\n\n\n\n1\nPART ONE\n\n\n9\nPART TWO\n\n\n16\nPART THREE\n\n\n26\nPART FOUR\n\n\n35\nPART FIVE"
  },
  {
    "objectID": "Projects/metal_in_mistborn.html#text-analysis",
    "href": "Projects/metal_in_mistborn.html#text-analysis",
    "title": "Metals in Mistborn",
    "section": "Text Analysis",
    "text": "Text Analysis\nWith the text structured, we analyze mentions of the Allomantic metals: iron, steel, tin, pewter, brass, zinc, copper, bronze, atium, and gold.\n\nTokenizing the Text\nUsing tidytext, we break the text into individual words:\n\ntext_df &lt;- df %&gt;%\n  unnest_tokens(word, text)\n\nhead(text_df) %&gt;% kable()\n\n\n\n\nchapter\nline_number\nchapter_id\nword\n\n\n\n\nPROLOGUE\n1\n1\nprologue\n\n\nPROLOGUE\n2\n1\nash\n\n\nPROLOGUE\n2\n1\nfell\n\n\nPROLOGUE\n2\n1\nfrom\n\n\nPROLOGUE\n2\n1\nthe\n\n\nPROLOGUE\n2\n1\nsky\n\n\n\n\n\n\n\nCounting Metal Mentions\nWe filter for the metals and count their mentions per chapter:\n\nmetals_list &lt;- c(\"iron\", \"steel\", \"tin\", \"pewter\", \"brass\", \"zinc\", \"copper\", \"bronze\", \"atium\", \"gold\")\nmetals_text &lt;- text_df %&gt;%\n  filter(word %in% metals_list) %&gt;%\n  mutate(word = str_to_title(word)) %&gt;%\n  count(chapter_id, word, name = \"mentions\") %&gt;%\n  left_join(part_lines, join_by(chapter_id == chapter_id)) %&gt;%\n  fill(part)\n\nhead(metals_text,4) %&gt;% kable()\n\n\n\n\nchapter_id\nword\nmentions\npart\n\n\n\n\n1\nIron\n1\nPART ONE\n\n\n1\nSteel\n2\nPART ONE\n\n\n1\nTin\n5\nPART ONE\n\n\n2\nGold\n1\nPART ONE\n\n\n\n\n\nThis produces a dataset with each metal’s mention count per chapter, linked to the book’s parts.\n\n\nStream graph\nThe stream graph was generated from the data above.\n\nstream_graph &lt;- ggplot() +\n  # Add part divider lines\n  geom_vline(\n    data = part_lines,\n    aes(xintercept = chapter_id),\n    color = \"grey50\",\n    linewidth = 0.2,\n    linetype = 2\n  ) +\n  # Streamgraph layer\n  geom_stream(\n    data = metals_text,\n    aes(x = chapter_id, y = mentions, fill = word),\n    type = \"mirror\",\n    extra_span = 0.1,\n    bw = 0.55\n  ) +\n  # Add part labels at the bottom of the plot\n  geom_text(\n    data = metals_text %&gt;%\n      group_by(part) %&gt;%\n      summarize(x = min(chapter_id) + n_distinct(chapter_id) / 2),\n    aes(x, y = -Inf, label = part),\n    vjust = -1,\n    hjust = 0.5,\n    color = \"grey60\",\n    family = \"Trajan Pro\",\n    size = 5\n  ) +\n  # Custom fill scale for metals\n  scale_fill_manual(values = metal_colors) +\n  labs(\n    fill = \"Metal\",\n    subtitle = \"Distribution of mentions share (number of words)&lt;br&gt;\n    per metal in each chapter. Parts are separated with vertical lines.\"\n  ) +\n  # Apply custom theme\n  theme_mistborn()\n\nstream_graph\n\n\n\n\n\n\n\n\nEach metal is a colored stream, with vertical lines marking the book’s parts. This highlights shifts in focus, like a spike in “atium” mentions during key plot moments.\n\n\nBar Plot\nA bar plot displays total mentions for each metal:\n\n# Summarize metal mention counts and reorder factors\nplot_data &lt;- metals_text %&gt;% \n  group_by(word) %&gt;% \n  summarise(total_mentions = sum(mentions)) %&gt;% \n  arrange(desc(total_mentions)) %&gt;% \n  mutate(word = fct_reorder(word, total_mentions))\n\n# Create horizontal bar plot\nbar_plot &lt;- plot_data %&gt;% \n  ggplot(aes(x = word, y = total_mentions, fill = word)) +\n  # Bar geometry\n  geom_col() +\n  # Custom fill colors\n  scale_fill_manual(values = metal_colors) +\n  # Remove legend\n  guides(fill = \"none\") +\n  # Labels\n  labs(y = \"Number of Mentions\") +\n  # Allow labels to extend past plot area\n  coord_cartesian(clip = \"off\") +\n  # Text labels above bars\n  geom_text(\n    aes(label = total_mentions), \n    color = \"grey70\",\n    hjust = 0.5,\n    vjust = -0.4,\n    size = 7.5,\n    family = \"Trajan Pro\"\n  ) +\n  # Apply custom theme\n  theme_mist() +\n  # Additional theme tweaks\n  theme(\n    axis.text.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_text(size = 18, angle = 90)\n  )\n\nbar_plot\n\n\n\n\n\n\n\n\n\n\nExploring Word Correlations\nTo see how metals are used in context, we calculate correlations between words appearing together. We flatten the text, split it into sentences, group sentences into sections of ten, and compute pairwise correlations for words appearing at least 20 times:\n\nmist_section &lt;- str_flatten(story_lines, collapse = \" \") %&gt;%\n  as_tibble() %&gt;%\n  unnest_sentences(sentence, value) %&gt;%\n  mutate(section = row_number() %/% 10) %&gt;%\n  unnest_tokens(word, sentence) %&gt;%\n  filter(!word %in% stop_words$word)\n\nword_cors &lt;- mist_section %&gt;%\n  group_by(word) %&gt;%\n  filter(n() &gt;= 20) %&gt;%\n  pairwise_cor(word, section, sort = TRUE) %&gt;%\n  filter(item1 %in% metals_list)\n\nhead(word_cors,4) %&gt;% kable()\n\n\n\n\nitem1\nitem2\ncorrelation\n\n\n\n\ntin\nenhanced\n0.3414959\n\n\npewter\nﬂared\n0.3408370\n\n\ncopper\nburning\n0.3076759\n\n\niron\nsteel\n0.2836302\n\n\n\n\n\nThis reveals which words are frequently associated with each metal.\n\n\nNetwork Graph\n\n# Set seed for reproducible layout\nset.seed(2000)\n\n# Filter word correlations and create graph\ngraph &lt;- word_cors %&gt;% \n  filter(correlation &gt; 0.15) %&gt;%\n  graph_from_data_frame()\n\n# Compute node degrees and tag metals\nV(graph)$degree &lt;- degree(graph)\nV(graph)$is_metal &lt;- V(graph)$name %in% metals_list  # Boolean for coloring nodes\n\n# Create network graph with conditional styling\nnetwork_Graph &lt;- graph %&gt;%\n  ggraph(layout = \"fr\") +\n  # Draw edges with thickness and transparency based on correlation\n  geom_edge_link(\n    aes(edge_alpha = correlation, edge_width = correlation), \n    color = \"grey40\",\n    show.legend = FALSE\n  ) +\n  # Draw nodes with size by degree and color by is_metal\n  geom_node_point(\n    aes(size = degree, color = is_metal)\n  ) +\n  # Custom colors for metal vs non-metal nodes\n  scale_color_manual(\n    values = c(\"FALSE\" = \"#ff79c6\", \"TRUE\" = \"lightblue\")\n  ) +\n  # Add labels with soft background\n  geom_node_label(\n    aes(label = name),\n    color = \"grey95\",\n    fill = \"#4D267380\",\n    repel = TRUE,\n    force = 1,\n    force_pull = 0.1,\n    family = \"Trajan Pro\",\n    label.size = 0,\n    size = 3.5\n  ) +\n  # Control node size range\n  scale_size_continuous(range = c(3, 10)) +\n  # Remove legends\n  guides(size = \"none\", color = \"none\") +\n  # Apply custom theme\n  theme_mistborn()\n\nnetwork_Graph\n\n\n\n\n\n\n\n\nMetals (in light blue) connect to related terms (in pink), showing context—like “steel” linking to “push.”\nDue to a desire for brevity I left some of the code out, to see the entire code you can view it on my github."
  },
  {
    "objectID": "Projects/metal_in_mistborn.html#conclusion",
    "href": "Projects/metal_in_mistborn.html#conclusion",
    "title": "Metals in Mistborn",
    "section": "Conclusion",
    "text": "Conclusion\nThroughout this project, I gained valuable experience and enjoyed the journey of exploration and experimentation. I tried numerous approaches and visualization options before settling on my final choices, and I’m quite pleased with how everything turned out."
  },
  {
    "objectID": "Projects/metal_in_mistborn.html#poster",
    "href": "Projects/metal_in_mistborn.html#poster",
    "title": "Metals in Mistborn",
    "section": "Poster",
    "text": "Poster\nThe final image was creating using canva to format and style it."
  },
  {
    "objectID": "Projects/Disc_Golf_Speed_Prediction.html",
    "href": "Projects/Disc_Golf_Speed_Prediction.html",
    "title": "Predicting Disc Golf Speed Ratings with Machine Learning",
    "section": "",
    "text": "Disc golf, a sport that combines precision, strategy, and physics, has surged in popularity, captivating players with its blend of athleticism and technical finesse. Central to the game is the disc itself, where the speed rating—a measure of how fast a disc must be thrown to achieve its intended flight path—plays a critical role in performance. As a avid disc golfer I wondered if I could predicting a disc’s speed rating based on its physical characteristics to better inform myself on the relationship between disc and glide rating and help me select discs in the future. In this project, I utilize machine learning to predict disc golf speed ratings using a dataset of disc specifications, employing the XGBoost algorithm to uncover patterns in the data."
  },
  {
    "objectID": "Projects/Disc_Golf_Speed_Prediction.html#introduction",
    "href": "Projects/Disc_Golf_Speed_Prediction.html#introduction",
    "title": "Predicting Disc Golf Speed Ratings with Machine Learning",
    "section": "",
    "text": "Disc golf, a sport that combines precision, strategy, and physics, has surged in popularity, captivating players with its blend of athleticism and technical finesse. Central to the game is the disc itself, where the speed rating—a measure of how fast a disc must be thrown to achieve its intended flight path—plays a critical role in performance. As a avid disc golfer I wondered if I could predicting a disc’s speed rating based on its physical characteristics to better inform myself on the relationship between disc and glide rating and help me select discs in the future. In this project, I utilize machine learning to predict disc golf speed ratings using a dataset of disc specifications, employing the XGBoost algorithm to uncover patterns in the data."
  },
  {
    "objectID": "Projects/Disc_Golf_Speed_Prediction.html#objective",
    "href": "Projects/Disc_Golf_Speed_Prediction.html#objective",
    "title": "Predicting Disc Golf Speed Ratings with Machine Learning",
    "section": "Objective",
    "text": "Objective\nThe goal is to build a predictive model that estimates a disc’s speed rating based on its physical attributes, such as rim width, diameter, and bead presence. By engineering features like rim-to-diameter ratio and disc area, and using a robust XGBoost model, I aim to achieve accurate predictions and identify which features most influence a disc’s speed. This analysis not only showcases the power of machine learning in sports analytics but also offers practical applications for disc design and selection."
  },
  {
    "objectID": "Projects/Disc_Golf_Speed_Prediction.html#data-preparation",
    "href": "Projects/Disc_Golf_Speed_Prediction.html#data-preparation",
    "title": "Predicting Disc Golf Speed Ratings with Machine Learning",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe dataset, found on kaggle, contains detailed measurements of various disc golf discs, including attributes like rim width, diameter, height, and bead presence. I used the Polars library for efficient data manipulation, dropping non-numeric flight characteristics (e.g., fade, glide, turn, stability) to focus on physical measurements and to avoid bais and lay the ground work for future models. To enrich the dataset, I engineered several features:\n\nRim ratio: Rim width divided by diameter, capturing the proportional width of the rim.\nProfile ratio: Height divided by diameter, reflecting the disc’s vertical profile.\nRim area: Product of rim width and rim depth, representing the rim’s surface area.\nDisc area: Calculated using the formula for the area of a circle, ((/2)^2).\nFlight plate ratio: Inside rim diameter divided by total diameter, indicating the proportion of the disc’s flight plate.\nInside rim width-depth product: Inside rim diameter multiplied by rim depth, capturing interaction effects.\n\nAdditionally, the categorical variable “DISC TYPE” was converted to dummy variables, and the “BEAD” column was transformed into a boolean (True for “Yes,” False for “No”). These transformations ensure the dataset is suitable for machine learning.\n\n\nShow the code\nimport polars as pl \nfrom lets_plot import *\nimport numpy as np \nimport polars.selectors as cs\nimport sklearn\n\nLetsPlot.setup_html()\n\ndf = pl.read_csv(\"disc-data.csv\", infer_schema_length=10000)\n\ndf = df.drop([\"MOLD\",\"FADE\",\"GLIDE\",\"TURN\",\"STABILITY\"])\ndf = df.to_dummies(\"DISC TYPE\")\n\ndf = df.with_columns(\n    pl.col(\"BEAD\").replace_strict({\"No\":False, \"Yes\":True}),\n    (pl.col(\"RIM WIDTH (cm)\") / pl.col(\"DIAMETER (cm)\")).alias(\"Rim ratio\"),\n    (pl.col(\"HEIGHT (cm)\")/pl.col(\"DIAMETER (cm)\")).alias(\"profile ratio\"),\n    (pl.col(\"RIM WIDTH (cm)\")*pl.col(\"RIM DEPTH (cm)\")).alias(\"rim_area\"),\n    (np.pi*(pl.col(\"DIAMETER (cm)\")/2)**2).alias(\"disc_area\"),\n    (pl.col(\"INSIDE RIM DIAMETER (cm)\")/ pl.col(\"DIAMETER (cm)\")).alias(\"flight_plate_ratio\"),\n    (pl.col(\"INSIDE RIM DIAMETER (cm)\")*pl.col(\"RIM DEPTH (cm)\")).alias(\"inside_rwd\"),\n)"
  },
  {
    "objectID": "Projects/Disc_Golf_Speed_Prediction.html#model-training",
    "href": "Projects/Disc_Golf_Speed_Prediction.html#model-training",
    "title": "Predicting Disc Golf Speed Ratings with Machine Learning",
    "section": "Model Training",
    "text": "Model Training\nI chose the XGBoost regressor, a powerful gradient-boosting algorithm, to predict speed ratings. The dataset was split into the standard training (70%) and testing (30%) sets to evaluate model performance. The model was configured with the following hyperparameters:\n\nBooster: DART (Dropout-meet-Additive Regression Trees) for improved generalization.\nNumber of estimators: 120, balancing complexity and performance.\nLearning rate (eta): 0.15, controlling the step size of updates.\nGamma: 3, enforcing sparsity in splits.\nMax depth: 8, limiting tree complexity to prevent overfitting.\nRate drop: 0.3, introducing dropout in DART to enhance robustness.\n\nThe model was trained on the training set, and predictions were made on the test set.\n\n\nShow the code\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(\"SPEED\")\ny = df[\"SPEED\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = xgb.XGBRegressor(\n    booster='dart',\n    n_estimators=120,\n    eta=0.15,\n    gamma=3,\n    max_depth=8,\n    rate_drop=0.3\n)\nmodel.fit(X_train, y_train)\n\n\nXGBRegressor(base_score=None, booster='dart', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eta=0.15, eval_metric=None,\n             feature_types=None, feature_weights=None, gamma=3,\n             grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=8, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=120, n_jobs=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressor?Documentation for XGBRegressoriFittedXGBRegressor(base_score=None, booster='dart', callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eta=0.15, eval_metric=None,\n             feature_types=None, feature_weights=None, gamma=3,\n             grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=8, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=120, n_jobs=None, ...)"
  },
  {
    "objectID": "Projects/Disc_Golf_Speed_Prediction.html#model-evaluation",
    "href": "Projects/Disc_Golf_Speed_Prediction.html#model-evaluation",
    "title": "Predicting Disc Golf Speed Ratings with Machine Learning",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nTo assess the model’s performance, I calculated several commen metrics:\n\nMean Squared Error (MSE): indicating the average squared difference between predicted and actual speed ratings.\nRoot Mean Squared Error (RMSE): showing the average prediction error in speed rating units.\nMean Absolute Error (MAE): reflecting the average absolute deviation of predictions.\nR-squared (R²): 0.96, suggesting that 96% of the variance in speed ratings is explained by the model.\n\nThese metrics indicate strong predictive performance, with the model capturing most of the variability in speed ratings while maintaining relatively low errors.\n\n\nShow the code\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\ny_pred = model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nmetrics = pl.DataFrame(\n    {\n        \"Metric\": [\"Mean Squared Error\",\"Root Mean Squared Error\",\"Mean Absolute Error\",\"R-squared\"],\n        \"Result\":[mse,rmse,mae,r2]\n    }\n)\n\nmetrics.select(\n    \"Metric\",\n    Result = pl.col(\"Result\").round(2)\n)\n# print(f\"Mean Squared Error: {mse:.2f}\")\n# print(f\"Root Mean Squared Error: {rmse:.2f}\")\n# print(f\"Mean Absolute Error: {mae:.2f}\")\n# print(f\"R-squared: {r2:.2f}\")\n\n\n\nshape: (4, 2)\n\n\n\nMetric\nResult\n\n\nstr\nf64\n\n\n\n\n\"Mean Squared Error\"\n0.55\n\n\n\"Root Mean Squared Error\"\n0.74\n\n\n\"Mean Absolute Error\"\n0.56\n\n\n\"R-squared\"\n0.96"
  },
  {
    "objectID": "Projects/Disc_Golf_Speed_Prediction.html#feature-importance",
    "href": "Projects/Disc_Golf_Speed_Prediction.html#feature-importance",
    "title": "Predicting Disc Golf Speed Ratings with Machine Learning",
    "section": "Feature Importance",
    "text": "Feature Importance\nTo understand which features drive speed predictions, I extracted the gain-based feature importance from the XGBoost model. The top five features, visualized in a bar plot, reveal the key drivers of speed ratings:\n\nRim ratio: The most influential feature, as wider rims typically correlate with higher speed ratings due to aerodynamic properties.\nflight_plate_ratio: The proportion of the flight plate relative to total diameter significantly affects speed and stability.\nDISC TYPE_Putter: Putter classification has a strong influence, as putters typically have lower speed ratings reflecting their design for control rather than distance.\nDISC TYPE_Driver: Driver classification significantly impacts speed predictions, as drivers are specifically designed for high-speed throws and distance.\nRIM WIDTH (cm): The physical rim width measurement captures direct aerodynamic properties affecting flight dynamics.\n\n\n\nShow the code\nimportance = model.get_booster().get_score(importance_type='gain')\nimportance_df = pl.DataFrame({\n    'Feature': list(importance.keys()),\n    'Importance': list(importance.values())\n}).with_columns(\n    pl.col('Importance').round(2)\n)\nimportance_df = importance_df.sort(\"Importance\", descending=True).head(5)\n\nplot1 = (\n    ggplot(importance_df, aes(y=as_discrete('Feature', order_by='Importance', order=1), x='Importance', fill='Feature')) +\n    geom_bar(stat=\"identity\") +\n    labs(title=\"Top Feature Importance (Gain)\", x=\"Importance Score\") +\n    guides(fill=\"none\") +\n    theme(\n        axis_text=element_text(size=12),\n        axis_title_y=element_blank()\n    )\n)\nplot1"
  },
  {
    "objectID": "Projects/Disc_Golf_Speed_Prediction.html#actual-vs.-predicted-speed",
    "href": "Projects/Disc_Golf_Speed_Prediction.html#actual-vs.-predicted-speed",
    "title": "Predicting Disc Golf Speed Ratings with Machine Learning",
    "section": "Actual vs. Predicted Speed",
    "text": "Actual vs. Predicted Speed\nTo visualize the model’s performance, I created a scatter plot comparing actual speed ratings to predicted values, with a red dashed line representing perfect predictions (y=x). The points, plotted with slight jitter to avoid overlap, cluster closely around the line, confirming the model’s accuracy. The plot, provides a clear visual of the model’s predictive ability.\n\n\nShow the code\nchart = pl.DataFrame({\n    \"actual\": y_test,\n    \"predicted\": y_pred\n})\n\nplot2 = (\n    ggplot(chart, aes(x=\"actual\", y=\"predicted\")) +\n    geom_point(position=\"jitter\", color=\"#1E90FF\", alpha=0.5) +\n    geom_abline(slope=1, intercept=0, color=\"red\", linetype=\"dashed\") +\n    labs(x=\"Actual Speed\", y=\"Predicted Speed\", title=\"Actual vs Predicted Speed\")\n)\nplot2"
  },
  {
    "objectID": "Projects/Disc_Golf_Speed_Prediction.html#conclusion",
    "href": "Projects/Disc_Golf_Speed_Prediction.html#conclusion",
    "title": "Predicting Disc Golf Speed Ratings with Machine Learning",
    "section": "Conclusion",
    "text": "Conclusion\nThe XGBoost model achieved an R² of 0.96, with rim ratio and related features emerging as the most influential predictors. These insights can help players select discs tailored to their throwing style. Future work could incorporate additional features, such as material properties or advanced aerodynamic metrics, to further refine predictions as well as additional models to predict the other commen metrics (i.e. Glide, Fade, Turn and Stability). This analysis underscores the intersection of data science and sports, offering a data-driven approach to understanding the physics of disc golf."
  },
  {
    "objectID": "resume.html#awards-recognition",
    "href": "resume.html#awards-recognition",
    "title": "Nathaniel Clark",
    "section": "Awards & Recognition",
    "text": "Awards & Recognition\n\n3rd Place, Cybersecurity Category — USU Hackathon 2025, Utah State University (Stenography Project)"
  }
]